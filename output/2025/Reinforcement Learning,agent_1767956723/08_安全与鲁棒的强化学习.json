{
  "name": "安全与鲁棒的强化学习",
  "paper_count": 27,
  "summary": "本类别关注强化学习在安全关键场景下的应用与保障机制。核心研究方向包括：研究智能体从受控训练环境向具有不同动态特性的真实环境迁移时，如何保证其行为的安全性，即使环境动态发生变化也能避免灾难性后果；开发能够提供可证明安全保证的鲁棒性方法。这类工作旨在解决标准强化学习因试错机制而不适用于安全关键应用（如机器人、自动驾驶）的根本问题，通过理论分析和算法设计，确保智能体在部署后能可靠、安全地运行。",
  "papers": [
    {
      "title": "Robust Transfer of Safety-Constrained Reinforcement Learning Agents",
      "abstract": "Reinforcement learning (RL) often relies on trial and error, which may cause undesirable outcomes. As a result, standard RL is inappropriate for safety-critical applications. To address this issue, one may train a safe agent in a controlled environment (where safety violations are allowed) and then transfer it to the real world (where safety violations may have disastrous consequences). Prior work has made this transfer safe as long as the new environment preserves the safety-related dynamics. However, in most practical applications, differences or shifts in dynamics between the two environments are inevitable, potentially leading to safety violations after the transfer. This work aims to guarantee safety even when the new environment has different (safety-related) dynamics. In other words, we aim to make the process of safe transfer robust. Our methodology (1) robustifies an agent in the controlled environment and (2) provably provides---under mild assumption---a safe transfer to new environments. The empirical evaluation shows that this method yields policies that are robust against changes in dynamics, demonstrating safety after transfer to a new environment.",
      "venue": "ICLR 2025",
      "authors": [
        "Markel Zubia",
        "Thiago D. Simão",
        "Nils Jansen"
      ],
      "paper_id": "rvXdGL4pCJ",
      "pdf_url": "https://openreview.net/pdf/ab9ae531a3eb122236bf4c5ea78b65a4436bec7c.pdf",
      "forum_url": "https://openreview.net/forum?id=rvXdGL4pCJ"
    },
    {
      "title": "Safety Representations for Safer Policy Learning",
      "abstract": "Reinforcement learning algorithms typically necessitate extensive exploration of the state space to find optimal policies. However, in safety-critical applications, the risks associated with such exploration can lead to catastrophic consequences. Existing safe exploration methods attempt to mitigate this by imposing constraints, which often result in overly conservative behaviours and inefficient learning. Heavy penalties for early constraint violations can trap agents in local optima, deterring exploration of risky yet high-reward regions of the state space. To address this, we introduce a method that explicitly learns state-conditioned safety representations. By augmenting the state features with these safety representations, our approach naturally encourages safer exploration without being excessively cautious, resulting in more efficient and safer policy learning in safety-critical scenarios. Empirical evaluations across diverse environments show that our method significantly improves task performance while reducing constraint violations during training, underscoring its effectiveness in balancing exploration with safety.",
      "venue": "ICLR 2025",
      "authors": [
        "Kaustubh Mani",
        "Vincent Mai",
        "Charlie Gauthier",
        "Annie S Chen",
        "Samer B. Nashed",
        "Liam Paull"
      ],
      "paper_id": "gJG4IPwg6l",
      "pdf_url": "https://openreview.net/pdf/9ffd37c94e309aed9028180f9adea0a773998d31.pdf",
      "forum_url": "https://openreview.net/forum?id=gJG4IPwg6l"
    },
    {
      "title": "HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents",
      "abstract": "Advancing safe autonomous systems through reinforcement learning (RL) requires robust benchmarks to evaluate performance, analyze methods, and assess agent competencies. Humans primarily rely on embodied visual perception to safely navigate and interact with their surroundings, making it a valuable capability for RL agents. However, existing vision-based 3D benchmarks only consider simple navigation tasks. To address this shortcoming, we introduce **HASARD**, a suite of diverse and complex tasks to **HA**rness **SA**fe **R**L with **D**oom, requiring strategic decision-making, comprehending spatial relationships, and predicting the short-term future. HASARD features three difficulty levels and two action spaces. An empirical evaluation of popular baseline methods demonstrates the benchmark's complexity, unique challenges, and reward-cost trade-offs. Visualizing agent navigation during training with top-down heatmaps provides insight into a method's learning process. Incrementally training across difficulty levels offers an implicit learning curriculum. HASARD is the first safe RL benchmark to exclusively target egocentric vision-based learning, offering a cost-effective and insightful way to explore the potential and boundaries of current and future safe RL methods. The environments and baseline implementations are open-sourced.",
      "venue": "ICLR 2025",
      "authors": [
        "Tristan Tomilin",
        "Meng Fang",
        "Mykola Pechenizkiy"
      ],
      "paper_id": "5BRFddsAai",
      "pdf_url": "https://openreview.net/pdf/fde61410f00ccf22d9510dc28b79cc37337c80d3.pdf",
      "forum_url": "https://openreview.net/forum?id=5BRFddsAai"
    },
    {
      "title": "On Minimizing Adversarial Counterfactual Error in Adversarial Reinforcement Learning",
      "abstract": "Deep Reinforcement Learning (DRL) policies are highly susceptible to adversarial noise in observations, which poses significant risks in safety-critical scenarios. The challenge inherent to adversarial perturbations is that by altering the information observed by the agent, the state becomes only partially observable. Existing approaches address this by either enforcing consistent actions across nearby states or maximizing the worst-case value within adversarially perturbed observations. However, the former suffers from performance degradation when attacks succeed, while the latter tends to be overly conservative, leading to suboptimal performance in benign settings. We hypothesize that these limitations stem from their failing to account for partial observability directly. To this end, we introduce a novel objective called Adversarial Counterfactual Error (ACoE), defined on the beliefs about the true state and balancing value optimization with robustness. To make ACoE scalable in model-free settings, we propose the theoretically-grounded surrogate objective Cumulative-ACoE (C-ACoE). Our empirical evaluations on standard benchmarks (MuJoCo, Atari, and Highway) demonstrate that our method significantly outperforms current state-of-the-art approaches for addressing adversarial RL challenges, offering a promising direction for improving robustness in DRL under adversarial conditions. Our code is available at https://github.com/romanbelaire/acoe-robust-rl.",
      "venue": "ICLR 2025",
      "authors": [
        "Roman Belaire",
        "Arunesh Sinha",
        "Pradeep Varakantham"
      ],
      "paper_id": "eUEMjwh5wK",
      "pdf_url": "https://openreview.net/pdf/c6c2c9a02baddce0d28ab6a51fb983fb1294e718.pdf",
      "forum_url": "https://openreview.net/forum?id=eUEMjwh5wK"
    },
    {
      "title": "Conflict-Averse Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning",
      "abstract": "In real-world applications, a reinforcement learning (RL) agent should consider multiple objectives and adhere to safety guidelines.\nTo address these considerations, we propose a constrained multi-objective RL algorithm named constrained multi-objective gradient aggregator (CoMOGA).\nIn the field of multi-objective optimization, managing conflicts between the gradients of the multiple objectives is crucial to prevent policies from converging to local optima.\nIt is also essential to efficiently handle safety constraints for stable training and constraint satisfaction.\nWe address these challenges straightforwardly by treating the maximization of multiple objectives as a constrained optimization problem (COP), where the constraints are defined to improve the original objectives.\nExisting safety constraints are then integrated into the COP, and the policy is updated by solving the COP, which ensures the avoidance of gradient conflicts.\nDespite its simplicity, CoMOGA guarantees convergence to global optima in a tabular setting.\nThrough various experiments, we have confirmed that preventing gradient conflicts is critical, and the proposed method achieves constraint satisfaction across all tasks.",
      "venue": "ICLR 2025",
      "authors": [
        "Dohyeong Kim",
        "Mineui Hong",
        "Jeongho Park",
        "Songhwai Oh"
      ],
      "paper_id": "ogXkmugNZw",
      "pdf_url": "https://openreview.net/pdf/424f31e4c737878b7bc3012c1852a66fe974a51f.pdf",
      "forum_url": "https://openreview.net/forum?id=ogXkmugNZw"
    },
    {
      "title": "ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially\nunsafe, interactions with their environments to learn effectively. These limitations\nconfine RL agents to simulated environments, hindering their ability to learn\ndirectly in real-world settings. In this work, we present ActSafe, a novel\nmodel-based RL algorithm for safe and efficient exploration. ActSafe learns\na well-calibrated probabilistic model of the system and plans optimistically\nw.r.t. the epistemic uncertainty about the unknown dynamics, while enforcing\npessimism w.r.t. the safety constraints. Under regularity assumptions on the\nconstraints and dynamics, we show that ActSafe guarantees safety during\nlearning while also obtaining a near-optimal policy in finite time. In addition, we\npropose a practical variant of ActSafe that builds on latest model-based RL advancements and enables safe exploration even in high-dimensional settings such\nas visual control. We empirically show that ActSafe obtains state-of-the-art\nperformance in difficult exploration tasks on standard safe deep RL benchmarks\nwhile ensuring safety during learning.",
      "venue": "ICLR 2025",
      "authors": [
        "Yarden As",
        "Bhavya Sukhija",
        "Lenart Treven",
        "Carmelo Sferrazza",
        "Stelian Coros",
        "Andreas Krause"
      ],
      "paper_id": "aKRADWBJ1I",
      "pdf_url": "https://openreview.net/pdf/973461224a77e45297db83e6414b350bc3a6c836.pdf",
      "forum_url": "https://openreview.net/forum?id=aKRADWBJ1I"
    },
    {
      "title": "Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning",
      "abstract": "Driven by inherent uncertainty and the sim-to-real gap, robust reinforcement learning (RL) seeks to improve resilience against the complexity and variability in agent-environment sequential interactions. Despite the existence of a large number of RL benchmarks, there is a lack of standardized benchmarks for robust RL. Current robust RL policies often focus on a specific type of uncertainty and are evaluated in distinct, one-off environments. In this work, we introduce Robust-Gymnasium, a unified modular benchmark designed for robust RL that supports a wide variety of disruptions across all key RL components—agents' observed state and reward, agents' actions, and the environment. Offering over sixty diverse task environments spanning control and robotics, safe RL, and multi-agent RL, it provides an open-source and user-friendly tool for the community to assess current methods and foster the development of robust RL algorithms. \nIn addition, we benchmark existing standard and robust RL algorithms within this framework, uncovering significant deficiencies in each and offering new insights.",
      "venue": "ICLR 2025",
      "authors": [
        "Shangding Gu",
        "Laixi Shi",
        "Muning Wen",
        "Ming Jin",
        "Eric Mazumdar",
        "Yuejie Chi",
        "Adam Wierman",
        "Costas Spanos"
      ],
      "paper_id": "2uQBSa2X4R",
      "pdf_url": "https://openreview.net/pdf/dba4a4a03ae16240b55d8098868a57fe27df9111.pdf",
      "forum_url": "https://openreview.net/forum?id=2uQBSa2X4R"
    },
    {
      "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
      "abstract": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time—assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific form of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent’s experience we aim to bridge the gap between offline RL and the complexity of real-world, non-stationary environments.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Suzan Ece Ada",
        "Georg Martius",
        "Emre Ugur",
        "Erhan Oztop"
      ],
      "paper_id": "24UJqxw1kv",
      "pdf_url": "https://openreview.net/pdf/6d72fb266c9975e3d778ba7818ae0999e5dc6469.pdf",
      "forum_url": "https://openreview.net/forum?id=24UJqxw1kv"
    },
    {
      "title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving",
      "abstract": "Assessing the safety of autonomous driving (AD) systems against security threats, particularly backdoor attacks, is a stepping stone for real-world deployment. However, existing works mainly focus on pixel-level triggers which are impractical to deploy in the real world. We address this gap by introducing a novel backdoor attack against the end-to-end AD systems that leverage one or more other vehicles' trajectories as triggers. To generate precise trigger trajectories, we first use temporal logic (TL) specifications to define the behaviors of attacker vehicles. Configurable behavior models are then used to generate these trajectories, which are quantitatively evaluated and iteratively refined based on the TL specifications. We further develop a negative training strategy by incorporating patch trajectories that are similar to triggers but are designated not to activate the backdoor. \nIt enhances the stealthiness of the attack and refines the system’s responses to trigger scenarios. \nThrough extensive experiments on 5 offline reinforcement learning (RL) driving agents with 6 trigger patterns and target actions combinations, we demonstrate the flexibility and effectiveness of our proposed attack, showing the under-exploration of existing end-to-end AD systems' vulnerabilities to such trajectory-based backdoor attacks.\nVideos of our attack are available at: https://sites.google.com/view/tlbackdoor/home.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Xuan Chen",
        "Shiwei Feng",
        "Zikang Xiong",
        "Shengwei An",
        "Yunshu Mao",
        "Lu Yan",
        "Guanhong Tao",
        "Wenbo Guo",
        "Xiangyu Zhang"
      ],
      "paper_id": "ifavjJaKQa",
      "pdf_url": "https://openreview.net/pdf/26313d9a67de648ba12a7315cbcaa87c2787a4f5.pdf",
      "forum_url": "https://openreview.net/forum?id=ifavjJaKQa"
    },
    {
      "title": "Confounding Robust Deep Reinforcement Learning: A Causal Approach",
      "abstract": "A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \\emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Mingxuan Li",
        "Junzhe Zhang",
        "Elias Bareinboim"
      ],
      "paper_id": "9fUr5iFU9j",
      "pdf_url": "https://openreview.net/pdf/f2c0349b151c96f7e468c2510103b9f6abb82afb.pdf",
      "forum_url": "https://openreview.net/forum?id=9fUr5iFU9j"
    },
    {
      "title": "HMARL-CBF – Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems",
      "abstract": "We address the problem of safe policy learning in multi-agent safety-critical autonomous systems.\nIn such systems, it is necessary for each agent to meet the safety requirements at all times while also cooperating with other agents to accomplish the task. Toward this end, we propose a safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier Functions (CBFs). Our proposed hierarchical approach decomposes the overall reinforcement learning problem into two levels –- learning joint cooperative behavior at the higher level and learning safe individual behavior at the lower or agent\nlevel conditioned on the high-level policy. Specifically, we propose a skill-based HMARL-CBF algorithm in which the higher-level problem involves learning a joint policy over the skills for all the agents and the lower-level problem involves\nlearning policies to execute the skills safely with CBFs. We validate our approach on challenging environment scenarios whereby a large number of agents have to safely navigate through conflicting road networks. Compared with existing state-of-the-art methods, our approach significantly improves the safety achieving near perfect (within $5\\%$) success/safety rate while also improving performance across all the environments.",
      "venue": "NeurIPS 2025",
      "authors": [
        "H M Sabbir Ahmad",
        "Ehsan Sabouni",
        "Alexander Wasilkoff",
        "Param Budhraja",
        "Zijian Guo",
        "Songyuan Zhang",
        "Chuchu Fan",
        "Christos Cassandras",
        "Wenchao Li"
      ],
      "paper_id": "fcLVqvyiqV",
      "pdf_url": "https://openreview.net/pdf/a131ed49c4d39f773fb55d932efc489957f59cc8.pdf",
      "forum_url": "https://openreview.net/forum?id=fcLVqvyiqV"
    },
    {
      "title": "Don’t Trade Off Safety: Diffusion Regularization for Constrained Offline RL",
      "abstract": "Constrained reinforcement learning (RL) seeks high-performance policies under safety constraints. We focus on an offline setting where the agent learns from a fixed dataset—a common requirement in realistic tasks to prevent unsafe exploration. To address this, we propose Diffusion-Regularized Constrained Offline Reinforcement Learning (DRCORL), which first uses a diffusion model to capture the behavioral policy from offline data and then extracts a simplified policy to enable efficient inference. We further apply gradient manipulation for safety adaptation, balancing the reward objective and constraint satisfaction. This approach leverages high-quality offline data while incorporating safety requirements. Empirical results show that DRCORL achieves reliable safety performance, fast inference, and strong reward outcomes across robot learning tasks. Compared to existing safe offline RL methods, it consistently meets cost limits and performs well with the same hyperparameters, indicating practical applicability in real-world scenarios. We open-source our implementation at https://github.com/JamesJunyuGuo/DRCORL.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Junyu Guo",
        "Zhi Zheng",
        "Donghao Ying",
        "Ming Jin",
        "Shangding Gu",
        "Costas Spanos",
        "Javad Lavaei"
      ],
      "paper_id": "eSIRst0WVy",
      "pdf_url": "https://openreview.net/pdf/9a447a7d6eaac537933edf1992b4ed998a5b87cf.pdf",
      "forum_url": "https://openreview.net/forum?id=eSIRst0WVy"
    },
    {
      "title": "Adaptable Safe Policy Learning from Multi-task Data with Constraint Prioritized Decision Transformer",
      "abstract": "Learning safe reinforcement learning (RL) policies from offline multi-task datasets without direct environmental interaction is crucial for efficient and reliable deployment of RL agents. Benefiting from their scalability and strong in-context learning capabilities, recent approaches attempt to utilize Decision Transformer (DT) architectures for offline safe RL, demonstrating promising adaptability across varying safety budgets. \nHowever, these methods primarily focus on single-constraint scenarios and struggle with diverse constraint configurations across multiple tasks. \nAdditionally, their reliance on heuristically defined Return-To-Go (RTG) inputs limits flexibility and reduces learning efficiency, particularly in complex multi-task environments. To address these limitations, we propose CoPDT, a novel DT-based framework designed to enhance adaptability to diverse constraints and varying safety budgets. Specifically, CoPDT introduces a constraint prioritized prompt encoder, which leverages sparse binary cost signals to accurately identify constraints, and a constraint prioritized Return-To-Go (CPRTG) token mechanism, which dynamically generates RTGs based on identified constraints and corresponding safety budgets. Extensive experiments on the OSRL benchmark demonstrate that CoPDT achieves superior efficiency and significantly enhanced safety compliance across diverse multi-task scenarios, surpassing state-of-the-art DT-based methods by satisfying safety constraints in more than twice as many tasks.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Ruiqi Xue",
        "Ziqian Zhang",
        "Lihe Li",
        "Cong Guan",
        "Lei Yuan",
        "Yang Yu"
      ],
      "paper_id": "HmsEHahtGx",
      "pdf_url": "https://openreview.net/pdf/2f87e4d5631143727eece93bbb5da6c0714dfd29.pdf",
      "forum_url": "https://openreview.net/forum?id=HmsEHahtGx"
    },
    {
      "title": "Automaton Constrained Q-Learning",
      "abstract": "Real-world robotic tasks often require agents to achieve sequences of goals while respecting time-varying safety constraints. However, standard Reinforcement Learning (RL) paradigms are fundamentally limited in these settings. A natural approach to these problems is to combine RL with Linear-time Temporal Logic (LTL), a formal language for specifying complex, temporally extended tasks and safety constraints. Yet, existing RL methods for LTL objectives exhibit poor empirical performance in complex and continuous environments. As a result, no scalable methods support both temporally ordered goals and safety simultaneously, making them ill-suited for realistic robotics scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm that addresses this gap by combining goal-conditioned value learning with automaton-guided reinforcement. ACQL supports most LTL task specifications and leverages their automaton representation to explicitly encode stage-wise goal progression and both stationary and non-stationary safety constraints. We show that ACQL outperforms existing methods across a range of continuous control tasks, including cases where prior methods fail to satisfy either goal-reaching or safety constraints. We further validate its real-world applicability by deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a cluttered, cabinet-like space with safety constraints. Our results demonstrate that ACQL is a robust and scalable solution for learning robotic behaviors according to rich temporal specifications.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Anastasios Manganaris",
        "Vittorio Giammarino",
        "Ahmed H Qureshi"
      ],
      "paper_id": "DLt2Ep1S3q",
      "pdf_url": "https://openreview.net/pdf/122b780c53647e31a8d1a02af71c8d62321edae5.pdf",
      "forum_url": "https://openreview.net/forum?id=DLt2Ep1S3q"
    },
    {
      "title": "Robust Reinforcement Learning in Finance:  Modeling Market Impact with Elliptic Uncertainty Sets",
      "abstract": "In financial applications, reinforcement learning (RL) agents are commonly trained on historical data, where their actions do not influence prices. However, during deployment, these agents trade in live markets where their own transactions can shift asset prices, a phenomenon known as market impact. This mismatch between training and deployment environments can significantly degrade performance.  Traditional robust RL approaches address this model misspecification by optimizing the worst-case performance over a set of uncertainties, but typically rely on symmetric structures that fail to capture the directional nature of market impact. To address this issue, we develop a novel class of elliptic uncertainty sets. We establish both implicit and explicit closed-form solutions for the worst-case uncertainty under these sets, enabling efficient and tractable robust policy evaluation. Experiments on single-asset and multi-asset trading tasks demonstrate that our method achieves superior Sharpe ratio and remains robust under increasing trade volumes, offering a more faithful and scalable approach to RL in financial markets.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Shaocong Ma",
        "Heng Huang"
      ],
      "paper_id": "IJGEtuVqwf",
      "pdf_url": "https://openreview.net/pdf/55d192c024283c95bafb543ef26f4a6b45c00e25.pdf",
      "forum_url": "https://openreview.net/forum?id=IJGEtuVqwf"
    },
    {
      "title": "VLMLight: Safety-Critical Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning Architecture",
      "abstract": "Traffic signal control (TSC) is a core challenge in urban mobility, where real-time decisions must balance efficiency and safety. Existing methods—ranging from rule-based heuristics to reinforcement learning (RL)—often struggle to generalize to complex, dynamic, and safety-critical scenarios. We introduce \\textbf{VLMLight}, a novel TSC framework that integrates vision-language meta-control with dual-branch reasoning. At the core of VLMLight is the first image-based traffic simulator that enables multi-view visual perception at intersections, allowing policies to reason over rich cues such as vehicle type, motion, and spatial density. A large language model (LLM) serves as a safety-prioritized meta-controller, selecting between a fast RL policy for routine traffic and a structured reasoning branch for critical cases. In the latter, multiple LLM agents collaborate to assess traffic phases, prioritize emergency vehicles, and verify rule compliance. Experiments show that VLMLight reduces waiting times for emergency vehicles by up to 65% over RL-only systems, while preserving real-time performance in standard conditions with less than 1% degradation. VLMLight offers a scalable, interpretable, and safety-aware solution for next-generation traffic signal control.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Maonan Wang",
        "Yirong Chen",
        "Aoyu Pang",
        "Yuxin Cai",
        "Chung Shue Chen",
        "Yuheng KAN",
        "Man On Pun"
      ],
      "paper_id": "08mjueZ0Iq",
      "pdf_url": "https://openreview.net/pdf/33b72708f460f7581306147471f9fba4a0d8fd81.pdf",
      "forum_url": "https://openreview.net/forum?id=08mjueZ0Iq"
    },
    {
      "title": "Diffusion Guided Adversarial State Perturbations in Reinforcement Learning",
      "abstract": "Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Xiaolin Sun",
        "Feidi Liu",
        "Zhengming Ding",
        "Zizhan Zheng"
      ],
      "paper_id": "Ix4or1zPZw",
      "pdf_url": "https://openreview.net/pdf/a970b436a5448e617cdc149b97a118d856c10272.pdf",
      "forum_url": "https://openreview.net/forum?id=Ix4or1zPZw"
    },
    {
      "title": "Efficient Safe Meta-Reinforcement Learning: Provable Near-Optimality and Anytime Safety",
      "abstract": "This paper studies the problem of safe meta-reinforcement learning (safe meta-RL), where an agent efficiently adapts to unseen tasks while satisfying safety constraints at all times during adaptation. We propose a framework consisting of two complementary modules: safe policy adaptation and safe meta-policy training. The first module introduces a novel one-step safe policy adaptation method that admits a closed-form solution, ensuring monotonic improvement, constraint satisfaction at every step, and high computational efficiency. The second module develops a Hessian-free meta-training algorithm that incorporates safety constraints on the meta-policy and leverages the analytical form of the adapted policy to enable scalable optimization. Together, these modules yield three key advantages over existing safe meta-RL methods: (i) superior optimality, (ii) anytime safety guarantee, and (iii) high computational efficiency. \nBeyond existing safe meta-RL analyses, we prove the anytime safety guarantee of policy adaptation and provide a lower bound of the expected total reward of the adapted policies compared with the optimal policies, which shows that the adapted policies are nearly optimal. Empirically, our algorithm achieves superior optimality, strict safety compliance, and substantial computational gains—up to 70\\% faster training and 50\\% faster testing—across diverse locomotion and navigation benchmarks.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Siyuan Xu",
        "Minghui Zhu"
      ],
      "paper_id": "ZtKXAbHQ43",
      "pdf_url": "https://openreview.net/pdf/f3b23bfef3678d11ea2e95b82e5d26432a105f6c.pdf",
      "forum_url": "https://openreview.net/forum?id=ZtKXAbHQ43"
    },
    {
      "title": "Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning",
      "abstract": "Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (*RTZ-VI-LCB*) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Na Li",
        "Zewu Zheng",
        "Wei Ni",
        "Hangguan Shan",
        "Wenjie Zhang",
        "Xinyu Li"
      ],
      "paper_id": "xVsC90U8yl",
      "pdf_url": "https://openreview.net/pdf/5e63d1e581de36695ab676873b8e9f3ba8e4a969.pdf",
      "forum_url": "https://openreview.net/forum?id=xVsC90U8yl"
    },
    {
      "title": "MOSDT: Self-Distillation-Based Decision Transformer for Multi-Agent Offline Safe Reinforcement Learning",
      "abstract": "We introduce MOSDT, the first algorithm designed for multi-agent offline safe reinforcement learning (MOSRL), alongside MOSDB, the first dataset and benchmark for this domain. Different from most existing knowledge distillation-based multi-agent RL methods, we propose policy self-distillation (PSD) with a new global information reconstruction scheme by fusing the observation features of all agents, streamlining training and improving parameter efficiency. We adopt full parameter sharing across agents, significantly slashing parameter count and boosting returns up to 38.4-fold by stabilizing training. We propose a new plug-and-play cost binary embedding (CBE) module, which binarizes cumulative costs as safety signals and embeds the signals into return features for efficient information aggregation. On the strong MOSDB benchmark, MOSDT achieves state-of-the-art (SOTA) returns in 14 out of 18 tasks (across all base environments including MuJoCo, Safety Gym, and Isaac Gym) while ensuring complete safety, with only 65% of the execution parameter count of a SOTA single-agent offline safe RL method CDT. Code, dataset, and results are available at this website: https://github.com/Lucian1115/MOSDT.git",
      "venue": "NeurIPS 2025",
      "authors": [
        "Yuchen Xia",
        "Yunjian Xu"
      ],
      "paper_id": "SYDmbsqHI0",
      "pdf_url": "https://openreview.net/pdf/ba9e5784427b11f66cb1caf1650cc37ca0217375.pdf",
      "forum_url": "https://openreview.net/forum?id=SYDmbsqHI0"
    },
    {
      "title": "Real-DRL: Teach and Learn in Reality",
      "abstract": "This paper introduces the Real-DRL framework for safety-critical autonomous systems, enabling runtime learning of a deep reinforcement learning (DRL) agent to develop safe and high-performance action policies in real plants while prioritizing safety. The Real-DRL consists of three interactive components: a DRL-Student, a PHY-Teacher, and a Trigger. The DRL-Student is a DRL agent that innovates in the dual self-learning and teaching-to-learn paradigm and the safety-status-dependent batch sampling. On the other hand, PHY-Teacher is a physics-model-based design of action policies that focuses solely on safety-critical functions. PHY-Teacher is novel in its real-time patch for two key missions: i) fostering the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of real plants. The Trigger manages the interaction between the DRL-Student and the PHY-Teacher. Powered by the three interactive components, the Real-DRL can effectively address safety challenges that arise from the unknown unknowns and the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety, ii) automatic hierarchy learning (i.e., safety-first learning and then high-performance learning), and iii) safety-informed batch sampling to address the experience imbalance caused by corner cases. Experiments with a real quadruped robot, a quadruped robot in Nvidia Isaac Gym, and a cart-pole system, along with comparisons and ablation studies, demonstrate the Real-DRL's effectiveness and unique features.",
      "venue": "NeurIPS 2025",
      "authors": [
        "Yanbing Mao",
        "Yihao Cai",
        "Lui Sha"
      ],
      "paper_id": "gXZlZAeqay",
      "pdf_url": "https://openreview.net/pdf/166edec4a124a1d5726e582880da1cce5b9d8169.pdf",
      "forum_url": "https://openreview.net/forum?id=gXZlZAeqay"
    },
    {
      "title": "Adversarial Inception Backdoor Attacks against Reinforcement Learning",
      "abstract": "Recent works have demonstrated the vulnerability of Deep Reinforcement Learning (DRL) algorithms against training-time, backdoor poisoning attacks. The objectives of these attacks are twofold: induce pre-determined, adversarial behavior in the agent upon observing a fixed trigger during deployment while allowing the agent to solve its intended task during training. Prior attacks assume arbitrary control over the agent's rewards, inducing values far outside the environment's natural constraints. This results in brittle attacks that fail once the proper reward constraints are enforced. Thus, in this work we propose a new class of backdoor attacks against DRL which are the first to achieve state of the art performance under strict reward constraints. These ``inception'' attacks manipulate the agent's training data -- inserting the trigger into prior observations and replacing high return actions with those of the targeted adversarial behavior. We formally define these attacks and prove they achieve both adversarial objectives against arbitrary Markov Decision Processes (MDP). Using this framework we devise an online inception attack which achieves an 100% attack success rate on multiple environments under constrained rewards while minimally impacting the agent's task performance.",
      "venue": "ICML 2025",
      "authors": [
        "Ethan Rathbun",
        "Alina Oprea",
        "Christopher Amato"
      ],
      "paper_id": "43529",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2025/poster/43529"
    },
    {
      "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking",
      "abstract": "Future advanced AI systems may learn sophisticated strategies through reinforcement learning (RL) that humans cannot understand well enough to safely evaluate. We propose a training method which avoids agents learning undesired multi-step plans that receive high reward (multi-step \"reward hacks\") even if humans are not able to detect that the behavior is undesired. The method, Myopic Optimization with Non-myopic Approval (MONA), works by combining short-sighted optimization with far-sighted reward. We demonstrate that MONA can prevent multi-step reward hacking that ordinary RL causes, even without being able to detect the reward hacking and without any extra information that ordinary RL does not get access to. We study MONA empirically in three settings which model different misalignment failure modes including 2-step environments with LLMs representing delegated oversight and encoded reasoning and longer-horizon gridworld environments representing sensor tampering.",
      "venue": "ICML 2025",
      "authors": [
        "Sebastian Farquhar",
        "Vikrant Varma",
        "David Lindner",
        "David Elson",
        "Caleb Biddulph",
        "Ian Goodfellow",
        "Rohin Shah"
      ],
      "paper_id": "44493",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2025/poster/44493"
    },
    {
      "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
      "abstract": "Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods.In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary.The implementation is available at https://github.com/nakanakakosuke/VALT_SAC.",
      "venue": "ICML 2025",
      "authors": [
        "Kosuke Nakanishi",
        "Akihiro Kubo",
        "Yuji Yasui",
        "Shin Ishii"
      ],
      "paper_id": "46516",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2025/poster/46516"
    },
    {
      "title": "PIGDreamer: Privileged Information Guided World Models for Safe Partially Observable Reinforcement Learning",
      "abstract": "Partial observability presents a significant challenge for safe reinforcement learning, as it impedes the identification of potential risks and rewards. Leveraging specific types of privileged information during training to mitigate the effects of partial observability has yielded notable empirical successes. In this paper, we propose Asymmetric Constrained Partially Observable Markov Decision Processes (ACPOMDPs) to theoretically examine the advantages of incorporating privileged information. Building upon ACPOMDPs, we propose the Privileged Information Guided Dreamer, a model-based safe reinforcement learning approach that leverages privileged information to enhance the agent's safety and performance through privileged representation alignment and an asymmetric actor-critic structure. Our empirical results demonstrate that our approach significantly outperforms existing methods in terms of safety and task-centric performance. Meanwhile, compared to alternative privileged model-based reinforcement learning methods, our approach exhibits superior performance and ease of training.",
      "venue": "ICML 2025",
      "authors": [
        "Dongchi Huang",
        "Jiaqi WANG",
        "Yang Li",
        "Chunhe Xia",
        "Tianle Zhang",
        "Kaige Zhang"
      ],
      "paper_id": "44134",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2025/poster/44134"
    },
    {
      "title": "Robust Offline Reinforcement Learning with Linearly Structured $f$-Divergence Regularization",
      "abstract": "The Robust Regularized Markov Decision Process (RRMDP) is proposed to learn policies robust to dynamics shifts by adding regularization to the transition dynamics in the value function. Existing methods mostly use unstructured regularization, potentially leading to conservative policies under unrealistic transitions. To address this limitation, we propose a novel framework, the $d$-rectangular linear RRMDP ($d$-RRMDP), which introduces latent structures into both transition kernels and regularization. We focus on offline reinforcement learning, where an agent learns policies from a precollected dataset in the nominal environment. We develop the Robust Regularized Pessimistic Value Iteration (R2PVI) algorithm that employs linear function approximation for robust policy learning in $d$-RRMDPs with $f$-divergence based regularization terms on transition kernels. We provide instance-dependent upper bounds on the suboptimality gap of R2PVI policies, demonstrating that these bounds are influenced by how well the dataset covers state-action spaces visited by the optimal robust policy under robustly admissible transitions. We establish information-theoretic lower bounds to verify that our algorithm is near-optimal. Finally, numerical experiments validate that R2PVI learns robust policies and exhibits superior computational efficiency compared to baseline methods.",
      "venue": "ICML 2025",
      "authors": [
        "Cheng Tang",
        "Zhishuai Liu",
        "Pan Xu"
      ],
      "paper_id": "45863",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2025/poster/45863"
    },
    {
      "title": "Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction",
      "abstract": "Off-dynamics reinforcement learning (RL), where training and deployment transition dynamics are different, can be formulated as learning in a robust Markov decision process (RMDP) where uncertainties in transition dynamics are imposed. Existing literature mostly assumes access to generative models allowing arbitrary state-action queries or pre-collected datasets with a good state coverage of the deployment environment, bypassing the challenge of exploration. In this work, we study a more realistic and challenging setting where the agent is limited to online interaction with the training environment. To capture the intrinsic difficulty of exploration in online RMDPs, we introduce the supremal visitation ratio, a novel quantity that measures the mismatch between the training dynamics and the deployment dynamics. We show that if this ratio is unbounded, online learning becomes exponentially hard. We propose the first computationally efficient algorithm that achieves sublinear regret in online RMDPs with $f$-divergence based transition uncertainties. We also establish matching regret lower bounds, demonstrating that our algorithm achieves optimal dependence on both the supremal visitation ratio and the number of interaction episodes. Finally, we validate our theoretical results through comprehensive numerical experiments.",
      "venue": "ICML 2025",
      "authors": [
        "Yiting He",
        "Zhishuai Liu",
        "Weixin Wang",
        "Pan Xu"
      ],
      "paper_id": "44017",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2025/poster/44017"
    }
  ]
}