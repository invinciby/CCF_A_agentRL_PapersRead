{
  "keyword": "Reinforcement Learning,agent",
  "provider": "DeepSeekProvider",
  "timestamp": 1767956723,
  "total_papers": 371,
  "categories": [
    {
      "name": "智能体技能设计与抽象表示",
      "paper_count": 30,
      "summary": "本类别聚焦于如何为智能体设计和表示复杂技能，使其能够执行高级任务。研究内容包括：利用大语言模型（LLM）从自然语言描述中自动生成技能奖励函数和代码（如MaestroMotif）；通过进化策略和强化学习自动生成具有内外骨骼结构的机器人形态与控制器；以及结合神经与符号方法（如Neuro-Symbolic Predicates、BlendRL）来学习抽象的世界模型和策略，以提高样本效率、泛化能力和可解释性。这些工作旨在构建能够理解、组合和执行复杂行为的更通用、更灵活的智能体。",
      "file": "01_智能体技能设计与抽象表示.json"
    },
    {
      "name": "强化学习算法优化与稳定性",
      "paper_count": 70,
      "summary": "本类别关注于改进深度强化学习（RL）算法的核心训练过程，以解决样本效率低、训练不稳定和非平稳环境适应等问题。具体研究方向包括：提出优先生成回放（Prioritized Generative Replay）方法，利用生成模型和相关性函数来高效复用和增强经验，防止过拟合；针对非平稳环境中的灾难性遗忘问题，提出局部约束策略优化（LCPO）方法，通过锚定旧经验来稳定在线学习；以及为了解决高更新数据比（UTD）训练中的价值函数泛化问题，引入模型增强数据（MAD-TD）来稳定训练并提升性能。这些工作旨在使RL算法更鲁棒、更高效。",
      "file": "02_强化学习算法优化与稳定性.json"
    },
    {
      "name": "模型与规划机制研究",
      "paper_count": 40,
      "summary": "本类别深入探索智能体内部的世界模型构建、规划机制及其可解释性。研究内容包括：通过概念可解释性方法，首次从机制上证明无模型RL智能体（如DRC）能够学习内部规划表示，其算法类似于并行双向搜索；为了在开放世界中进行长视野探索，提出长短时想象（LS-Imagine）方法，构建能够模拟目标条件跳转状态转移的世界模型；以及为了提升基于Transformer的世界模型性能，引入行动条件对比预测编码（TWISTER）来学习高层时序特征表示，在Atari等基准上取得领先成绩。这些工作致力于理解和提升智能体的内部推理与长远规划能力。",
      "file": "03_模型与规划机制研究.json"
    },
    {
      "name": "多智能体强化学习与博弈论",
      "paper_count": 84,
      "summary": "本类别研究多智能体系统中的交互、合作与均衡问题，结合强化学习与博弈论原理。核心方向包括：为解决多智能体强化学习（MARL）中纳什均衡等解概念计算难的问题，引入行为经济学概念（风险厌恶和有限理性），提出易于计算的风险厌恶量化响应均衡（RQE），并分析了其样本复杂度；以及为了在一般和博弈中找到社会有益的均衡，从第一性原理推导出优势对齐（Advantage Alignment）算法族，通过对齐智能体间的优势信号来促进合作，该方法简化了对手塑造的数学表述并扩展至连续动作域。这些工作旨在为多智能体系统提供可计算、可合作的理论与算法基础。",
      "file": "04_多智能体强化学习与博弈论.json"
    },
    {
      "name": "通用智能体与大规模预训练",
      "paper_count": 27,
      "summary": "本类别探索通过大规模、多样化的任务训练来获得具有广泛物理推理和零样本泛化能力的通用智能体。代表性工作Kinetix构建了一个开放的、基于物理的2D控制任务空间，并利用硬件加速的物理引擎生成了数千万个任务用于训练一个通用的RL智能体。该智能体展现出强大的物理推理能力，能够零样本解决未见过的、由人类设计的环境，并且在特定任务上微调后性能显著优于从头训练的智能体。这项研究证明了为在线RL进行大规模、混合质量预训练的可行性，是迈向通用序列决策智能体的重要一步。",
      "file": "05_通用智能体与大规模预训练.json"
    },
    {
      "name": "因果强化学习与反事实推理",
      "paper_count": 8,
      "summary": "本类别研究如何将因果推理，特别是反事实推理，整合到强化学习与决策框架中，以超越传统的观测和干预策略。核心工作“反事实可实现性”为反事实分布的可实现性（即能否通过物理实验直接采样）提供了形式化定义和完备判定算法。该框架突破了Pearl因果层级中第三层（反事实）通常不可及的认知，证明了在因果公平和因果强化学习等场景下，反事实数据收集策略在理论上优于纯粹的干预或观测策略。这为开发能够进行反事实思考、做出更优决策的智能体奠定了新的理论基础。",
      "file": "06_因果强化学习与反事实推理.json"
    },
    {
      "name": "具身智能与人机交互",
      "paper_count": 14,
      "summary": "本类别关注具身智能体（如机器人）在动态、社交化环境中的感知、决策与交互问题，特别是与人类的协作。代表性研究“Following the Human Thread in Social Navigation”针对社交导航中机器人需要实时适应人类运动轨迹的挑战，提出了首个基于智能体状态-动作历史来推断社会动态的模型（SDA）。该研究采用两阶段强化学习框架：第一阶段在拥有完全轨迹信息（特权信息）的条件下学习编码社会动态并训练策略；第二阶段策略仅依靠自身历史状态和动作实时推断社会动态来操作。在Habitat 3.0平台上的测试表明，该方法在寻找和跟随人类任务上达到了新的最先进性能。",
      "file": "07_具身智能与人机交互.json"
    },
    {
      "name": "安全与鲁棒的强化学习",
      "paper_count": 27,
      "summary": "本类别关注强化学习在安全关键场景下的应用与保障机制。核心研究方向包括：研究智能体从受控训练环境向具有不同动态特性的真实环境迁移时，如何保证其行为的安全性，即使环境动态发生变化也能避免灾难性后果；开发能够提供可证明安全保证的鲁棒性方法。这类工作旨在解决标准强化学习因试错机制而不适用于安全关键应用（如机器人、自动驾驶）的根本问题，通过理论分析和算法设计，确保智能体在部署后能可靠、安全地运行。",
      "file": "08_安全与鲁棒的强化学习.json"
    },
    {
      "name": "智能体对齐与价值学习",
      "paper_count": 23,
      "summary": "本类别聚焦于如何将智能体（特别是基于大语言模型的智能体）的行为与人类价值观、伦理原则或特定目标进行对齐。研究内容包括：设计透明、可解释的奖励函数来显式编码人类道德价值观（如道义论和功利主义），并通过强化学习对基础智能体模型进行微调以实现道德对齐；探索如何利用内在奖励使智能体“忘记”先前习得的自私策略，并评估所学道德策略在不同环境中的泛化能力。该方向旨在解决智能体日益强大且不透明所带来的对齐挑战，为当前主要依赖人类偏好数据（如RLHF/DPO）的主流对齐方法提供一种更透明、可能更具成本效益的替代方案。",
      "file": "09_智能体对齐与价值学习.json"
    },
    {
      "name": "联邦与分布式强化学习",
      "paper_count": 8,
      "summary": "本类别聚焦于在数据分散、设备异构或隐私敏感的场景下，如何高效、协同地训练强化学习智能体。核心研究方向包括：提出异步分布式强化学习框架（如DistRL），通过集中训练与分散数据采集来优化移动设备上控制智能体的在线微调效率；研究个性化联邦强化学习（如PFedRL-Rep），利用共享表征与个性化权重来应对不同智能体面临的异构环境，并理论证明其线性加速收敛。这些工作旨在解决现实部署中数据孤岛、计算资源受限和隐私保护等挑战，推动强化学习从集中式仿真走向分布式实际应用。",
      "file": "10_联邦与分布式强化学习.json"
    },
    {
      "name": "智能体引导与激励机制设计",
      "paper_count": 1,
      "summary": "本类别研究如何通过设计外部奖励或激励机制，在不确定其他智能体学习动态的情况下，引导多智能体系统朝向期望的策略或行为。区别于传统的多智能体协同或竞争，它侧重于一个中央引导者（如平台、机制设计者）如何主动施加影响来塑造群体行为。代表性工作考虑了更一般的“马尔可夫智能体”学习动态，并采用基于模型的非回合制强化学习来学习历史依赖的引导策略，以处理对智能体学习动态的模型不确定性。该方向在经济学、自动驾驶、机器人集群控制等领域有广泛应用，旨在实现高效、低成本的系统级目标。",
      "file": "11_智能体引导与激励机制设计.json"
    },
    {
      "name": "强化学习应用与系统",
      "paper_count": 24,
      "summary": "本类别关注强化学习在特定复杂现实问题（如云计算、自动化任务生成）中的实际应用与系统构建。研究内容包括：针对动态工作流调度等具体领域挑战，设计结合图神经网络和离线-在线学习的专用DRL框架；以及开发能够自动生成多样化、可学习且有趣的环境与奖励函数的开放生成框架，以推动自我改进的AI系统发展。这些工作侧重于将RL核心算法与领域知识、系统设计相结合，以解决实际部署中的效率、适应性和可扩展性问题。",
      "file": "12_强化学习应用与系统.json"
    },
    {
      "name": "智能体行为分析与可解释性",
      "paper_count": 4,
      "summary": "本类别聚焦于运用来自神经科学、动物行为学等领域的分析工具，对深度强化学习智能体的行为模式、内部表示和决策机制进行深入、定量的研究，旨在超越简单的奖励曲线比较，揭示智能体在复杂、开放环境中的结构化行为、隐式规划和涌现的动态。研究内容包括：设计新颖的复杂环境（如ForageWorld）来模拟现实世界的挑战（如稀疏资源、捕食者威胁）；开发结合行为分析和神经分析（如RNN动力学分析）的通用框架，以诊断智能体的策略、记忆和规划能力；探索模型无关智能体如何通过涌现动态表现出类似规划的行为，而无需显式的世界模型或记忆模块。此类研究对于理解日益复杂的智能体、确保其安全对齐以及最大化难以通过奖励衡量的期望行为至关重要。",
      "file": "13_智能体行为分析与可解释性.json"
    },
    {
      "name": "强化学习理论基础与可解释性",
      "paper_count": 4,
      "summary": "本类别聚焦于强化学习及深度学习中核心组件的理论分析与可解释性研究，旨在从数学和机制层面理解模型的工作原理、表示能力和泛化行为。研究内容包括：通过“交互实体”的视角（如多智能体系统中的智能体、基因序列中的等位基因）分析自注意力机制如何高效地表征、学习和泛化实体间的成对交互函数，揭示其作为“互交互学习器”的本质；并基于理论洞察设计新的神经网络模块（如HyperFeatureAttention, HyperAttention）以捕获更复杂的多实体依赖关系。此类研究不局限于特定算法优化，而是为智能体的核心计算单元提供理论基础和设计原则。",
      "file": "14_强化学习理论基础与可解释性.json"
    },
    {
      "name": "检索增强生成与多智能体系统",
      "paper_count": 3,
      "summary": "本类别聚焦于将多智能体系统框架应用于检索增强生成（RAG）等复杂任务，以优化和协调不同组件（如检索器与大语言模型）之间的交互。研究内容主要包括：设计轻量级的多智能体系统作为代理框架，在不修改核心组件的前提下，通过智能体间的协作（如评估检索需求、生成查询、筛选信息）来提升整体任务性能；开发适用于此类多智能体协调的强化学习训练机制（如树状结构奖励分配）。该方向旨在解决独立开发组件间的对齐难题，并借鉴人类交互行为模式，以实现更灵活、高效且泛化能力强的智能系统。",
      "file": "15_检索增强生成与多智能体系统.json"
    },
    {
      "name": "状态与观测表示学习",
      "paper_count": 4,
      "summary": "本类别聚焦于强化学习智能体如何处理和适应动态变化的状态或观测空间。核心挑战在于当环境的状态特征因传感器维护、升级或故障而发生变化时，如何保证已部署策略的性能不出现灾难性下降。研究内容包括：形式化定义状态可演化强化学习问题；通过复用旧策略的行为经验进行离线学习，以快速适应新状态空间；构建状态重建模型以处理传感器缺失；以及设计自动集成机制来动态组合新旧策略，从而在状态空间演变时实现高效、鲁棒的策略迁移与重用，避免代价高昂的重新探索。",
      "file": "16_状态与观测表示学习.json"
    }
  ]
}