{
  "name": "强化学习应用与特定领域方法",
  "paper_count": 15,
  "summary": "该类别专注于强化学习在特定领域或具体问题上的应用研究，以及为解决这些领域独特挑战而设计的算法。研究重点在于将RL方法应用于如芯片设计（逻辑合成）、医疗决策等复杂现实世界任务，并针对领域特性（如搜索空间结构、决策可解释性需求）设计定制化的学习框架、奖励机制或训练流程。相关工作旨在验证RL在专业领域的实用性，并探索如何将领域知识有效融入学习过程以提升性能、效率或决策质量。",
  "papers": [
    {
      "title": "Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization",
      "abstract": "Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (``synthesis recipe\"), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of hardware design complexities — from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) — requires a nuanced 'synthesis recipe' guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We present ABC-RL, a meticulously tuned $\\alpha$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process. Computed based on similarity scores through nearest neighbor retrieval from the training dataset, ABC-RL yields superior synthesis recipes tailored for a wide array of hardware designs. Our findings showcase substantial enhancements in the Quality of Result (QoR) of synthesized circuits, boasting improvements of up to 24.8\\% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies.",
      "venue": "ICLR 2024",
      "authors": [
        "Animesh Basak Chowdhury",
        "Marco Romanelli",
        "Benjamin Tan",
        "Ramesh Karri",
        "Siddharth Garg"
      ],
      "paper_id": "19604",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/19604"
    },
    {
      "title": "Reward Design for Justifiable Sequential Decision-Making",
      "abstract": "Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",
      "venue": "ICLR 2024",
      "authors": [
        "Aleksa Sukovic",
        "Goran Radanovic"
      ],
      "paper_id": "18740",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/18740"
    },
    {
      "title": "Advancing DRL Agents in Commercial Fighting Games: Training, Integration, and Agent-Human Alignment",
      "abstract": "Deep Reinforcement Learning (DRL) agents have demonstrated impressive success in a wide range of game genres. However, existing research primarily focuses on optimizing DRL competence rather than addressing the challenge of prolonged player interaction. In this paper, we propose a practical DRL agent system for fighting games named _Shūkai_, which has been successfully deployed to Naruto Mobile, a popular fighting game with over 100 million registered users. _Shūkai_ quantifies the state to enhance generalizability, introducing Heterogeneous League Training (HELT) to achieve balanced competence, generalizability, and training efficiency. Furthermore, _Shūkai_ implements specific rewards to align the agent's behavior with human expectations. _Shūkai_'s ability to generalize is demonstrated by its consistent competence across all characters, even though it was trained on only 13% of them. Additionally, HELT exhibits a remarkable 22% improvement in sample efficiency. _Shūkai_ serves as a valuable training partner for players in Naruto Mobile, enabling them to enhance their abilities and skills.",
      "venue": "ICML 2024",
      "authors": [
        "Chen Zhang",
        "Qiang HE",
        "Yuan Zhou",
        "Elvis S. Liu",
        "Hong Wang",
        "Jian Zhao",
        "Yang Wang"
      ],
      "paper_id": "33504",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/33504"
    },
    {
      "title": "A Hierarchical Adaptive Multi-Task Reinforcement Learning Framework for Multiplier Circuit Design",
      "abstract": "Multiplier design---which aims to explore a large combinatorial design space to simultaneously optimize multiple conflicting objectives---is a fundamental problem in the integrated circuits industry. Although traditional approaches tackle the multi-objective multiplier optimization problem by manually designed heuristics, reinforcement learning (RL) offers a promising approach to discover high-speed and area-efficient multipliers. However, the existing RL-based methods struggle to find Pareto-optimal circuit designs for all possible preferences, i.e., weights over objectives, in a sample-efficient manner. To address this challenge, we propose a novel hierarchical adaptive (HAVE) multi-task reinforcement learning framework. The hierarchical framework consists of a meta-agent to generate diverse multiplier preferences, and an adaptive multi-task agent to collaboratively optimize multipliers conditioned on the dynamic preferences given by the meta-agent. To the best of our knowledge, HAVE is the first to well approximate Pareto-optimal circuit designs for the entire preference space with high sample efficiency. Experiments on multipliers across a wide range of input widths demonstrate that HAVE significantly Pareto-dominates state-of-the-art approaches, achieving up to 28% larger hypervolume. Moreover, experiments demonstrate that multipliers designed by HAVE can well generalize to large-scale computation-intensive circuits.",
      "venue": "ICML 2024",
      "authors": [
        "Zhihai Wang",
        "Jie Wang",
        "Dongsheng Zuo",
        "Ji Yunjie",
        "Xilin Xia",
        "Yuzhe Ma",
        "Jianye Hao",
        "Mingxuan Yuan",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "paper_id": "34306",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34306"
    },
    {
      "title": "DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max Vehicle Routing Problems",
      "abstract": "The min-max vehicle routing problem (min-max VRP) traverses all given customers by assigning several routes and aims to minimize the length of the longest route. Recently, reinforcement learning (RL)-based sequential planning methods have exhibited advantages in solving efficiency and optimality. However, these methods fail to exploit the problem-specific properties in learning representations, resulting in less effective features for decoding optimal routes. This paper considers the sequential planning process of min-max VRPs as two coupled optimization tasks: customer partition for different routes and customer navigation in each route (i.e., partition and navigation). To effectively process min-max VRP instances, we present a novel attention-based Partition-and-Navigation encoder (P&N Encoder) that learns distinct embeddings for partition and navigation. Furthermore, we utilize an inherent symmetry in decoding routes and develop an effective agent-permutation-symmetric (APS) loss function. Experimental results demonstrate that the proposed Decoupling-Partition-Navigation (DPN) method significantly surpasses existing learning-based methods in both single-depot and multi-depot min-max VRPs. Our code is available at",
      "venue": "ICML 2024",
      "authors": [
        "zhi Zheng",
        "Shunyu Yao",
        "Zhenkun Wang",
        "Tong Xialiang",
        "Mingxuan Yuan",
        "Ke Tang"
      ],
      "paper_id": "33667",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/33667"
    },
    {
      "title": "Reinforcement Learning within Tree Search for Fast Macro Placement",
      "abstract": "Macro placement is a crucial step in modern chip design, and reinforcement learning (RL) has recently emerged as a promising technique for improving the placement quality. However, existing RL-based techniques are hindered by their low sample efficiency, requiring numerous online rollouts or substantial offline expert data to achieve bootstrap, which are often impractical in industrial scenarios. To address this challenge, we propose a novel sample-efficient framework, namely **EfficientPlace**, for fast macro placement. EfficientPlace integrates a global tree search algorithm to strategically direct the optimization process, as well as a RL agent for local policy learning to advance the tree search. Experiments on commonly used benchmarks demonstrate that EfficientPlace achieves remarkable placement quality within a short timeframe, outperforming recent state-of-the-art approaches.",
      "venue": "ICML 2024",
      "authors": [
        "Zijie Geng",
        "Jie Wang",
        "Ziyan Liu",
        "Siyuan Xu",
        "Zhentao Tang",
        "Mingxuan Yuan",
        "Jianye Hao",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "paper_id": "34772",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34772"
    },
    {
      "title": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation",
      "abstract": "Deep reinforcement learning (DRL) is playing an increasingly important role in real-world applications. However, obtaining an optimally performing DRL agent for complex tasks, especially with sparse rewards, remains a significant challenge. The training of a DRL agent can be often trapped in a bottleneck without further progress. In this paper, we propose RICE, an innovative refining scheme for reinforcement learning that incorporates explanation methods to break through the training bottlenecks. The high-level idea of RICE is to construct a new initial state distribution that combines both the default initial states and critical states identified through explanation methods, thereby encouraging the agent to explore from the mixed initial states. Through careful design, we can theoretically guarantee that our refining scheme has a tighter sub-optimality bound. We evaluate RICE in various popular RL environments and real-world applications. The results demonstrate that RICE significantly outperforms existing refining schemes in enhancing agent performance.",
      "venue": "ICML 2024",
      "authors": [
        "Zelei Cheng",
        "Xian Wu",
        "Jiahao Yu",
        "Sabrina Yang",
        "Gang Wang",
        "Xinyu Xing"
      ],
      "paper_id": "34129",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34129"
    },
    {
      "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
      "abstract": "We present RoboGen, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation. RoboGen leverages the latest advancements in foundation and generative models. Instead of directly adapting these models to produce policies or low-level actions, we advocate for a generative scheme, which uses these models to automatically generate diversified tasks, scenes, and training supervisions, thereby scaling up robotic skill learning with minimal human supervision. Our approach equips a robotic agent with a self-guided propose-generate-learn cycle: the agent first proposes interesting tasks and skills to develop, and then generates simulation environments by populating pertinent assets with proper spatial configurations. Afterwards, the agent decomposes the proposed task into sub-tasks, selects the optimal learning approach (reinforcement learning, motion planning, or trajectory optimization), generates required training supervision, and then learns policies to acquire the proposed skill. Our fully generative pipeline can be queried repeatedly, producing an endless stream of skill demonstrations associated with diverse tasks and environments.",
      "venue": "ICML 2024",
      "authors": [
        "Yufei Wang",
        "Zhou Xian",
        "Feng Chen",
        "Johnson Tsun-Hsuan Wang",
        "Yian Wang",
        "Katerina Fragkiadaki",
        "Zackory Erickson",
        "David Held",
        "Chuang Gan"
      ],
      "paper_id": "34008",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34008"
    },
    {
      "title": "Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning",
      "abstract": "Cultural accumulation drives the open-ended and diverse progress in capabilities spanning human history. It builds an expanding body of knowledge and skills by combining individual exploration with inter-generational information transmission. Despite its widespread success among humans, the capacity for artificial learning agents to accumulate culture remains under-explored. In particular, approaches to reinforcement learning typically strive for improvements over only a single lifetime. Generational algorithms that do exist fail to capture the open-ended, emergent nature of cultural accumulation, which allows individuals to trade-off innovation and imitation. Building on the previously demonstrated ability for reinforcement learning agents to perform social learning, we find that training setups which balance this with independent learning give rise to cultural accumulation. These accumulating agents outperform those trained for a single lifetime with the same cumulative experience. We explore this accumulation by constructing two models under two distinct notions of a generation: episodic generations, in which accumulation occurs via in-context learning and train-time generations, in which accumulation occurs via in-weights learning. In-context and in-weights cultural accumulation can be interpreted as analogous to knowledge and skill accumulation, respectively. To the best of our knowledge, this work is the first to present general models that achieve emergent cultural accumulation in reinforcement learning, opening up new avenues towards more open-ended learning systems, as well as presenting new opportunities for modelling human culture.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Jonathan Cook",
        "Chris Lu",
        "Edward Hughes",
        "Joel Leibo",
        "Jakob Foerster"
      ],
      "paper_id": "93559",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/93559"
    },
    {
      "title": "Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems",
      "abstract": "Collaborative robots and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity and enhancing safety. Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human’s actions to act independently rather than in a synergistic fashion. To remedy this deficiency, we develop HMT approaches that enable iterative, mixed-initiative team development allowing end-users to interactively reprogram interpretable AI teammates. Our 50-subject study provides several findings that we summarize into guidelines. While all approaches underperform a simple collaborative heuristic (a critical, negative result for learning-based methods), we find that white-box approaches supported by interactive modification can lead to significant team development, outperforming white-box approaches alone, and that black-box approaches are easier to train and result in better HMT performance highlighting a tradeoff between explainability and interactivity versus ease-of-training. Together, these findings present three important future research directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Rohan Paleja",
        "Michael Munje",
        "Kimberlee Chang",
        "Reed Jensen",
        "Matthew Gombolay"
      ],
      "paper_id": "94740",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94740"
    },
    {
      "title": "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning",
      "abstract": "Pre-trained vision language models (VLMs), though powerful, typically lack training on decision-centric data, rendering them sub-optimal for decision-making tasks such as in-the-wild device control through Graphical User Interfaces (GUIs) when used off-the-shelf. While training with static demonstrations has shown some promise, we show that such methods fall short when controlling real GUIs due to their failure to deal with real world stochasticity and dynamism not captured in static observational data. This paper introduces a novel autonomous RL approach, called DigiRL, for training in-the-wild device control agents through fine-tuning a pre-trained VLM in two stages: offline and offline-to-online RL. We first build a scalable and parallelizable Android learning environment equipped with a VLM-based general-purpose evaluator and then identify the key design choices for simple and effective RL in this domain. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild (AitW) dataset, where our 1.5B VLM trained with RL achieves a 49.5\\% absolute improvement -- from 17.7 to 67.2\\% success rate -- over supervised fine-tuning with static human demonstration data. It is worth noting that such improvement is achieved without any additional supervision or demonstration data. These results significantly surpass not only the prior best agents, including AppAgent with GPT-4V (8.3\\% success rate) and the 17B CogAgent trained with AitW data (14.4\\%), but also our implementation of prior best autonomous RL approach based on filtered behavior cloning (57.8\\%), thereby establishing a new state-of-the-art for digital agents for in-the-wild device control.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Hao Bai",
        "Yifei Zhou",
        "Jiayi Pan",
        "Mert Cemri",
        "Alane Suhr",
        "Sergey Levine",
        "Aviral Kumar"
      ],
      "paper_id": "96658",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/96658"
    },
    {
      "title": "Explaining RL Decisions with Trajectories': A Reproducibility Study",
      "abstract": "This work investigates the reproducibility of the paper \"Explaining RL decisions with trajectories“ by Deshmukh et al. (2023). The original paper introduces a novel approach in explainable reinforcement learning based on the attribution decisions of an agent to specific clusters of trajectories encountered during training. We verify the main claims from the paper, which state that (i) training on less trajectories induces a lower initial state value, (ii) trajectories in a cluster present similar high-level patterns, (iii) distant trajectories influence the decision of an agent, and (iv) humans correctly identify the attributed trajectories to the decision of the agent. We recover the environments used by the authors based on the partial original code they provided for one of the environments (Grid-World), and implemented the remaining from scratch (Seaquest and HalfCheetah, Breakout, Q*Bert).\nWhile we confirm that (i), (ii), and (iii) partially hold, we extend on the largely qualitative experiments from the authors by introducing a quantitative metric to further support (iii), and new experiments and visual results for (i). Moreover, we investigate the use of different clustering algorithms and encoder architectures to further support (ii). We could not support (iv), given the limited extent of the original experiments. We conclude that, while some of the claims can be supported, further investigations and experiments could be of interest. We recognize the novelty of the work from the authors and hope that our work paves the way for clearer and more transparent approaches.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Karim Abdel Sadek",
        "Matteo Nulli",
        "Joan Velja",
        "Jort Vincenti"
      ],
      "paper_id": "99336",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/99336"
    },
    {
      "title": "Hybrid Reinforcement Learning Breaks Sample Size Barriers In Linear MDPs",
      "abstract": "Hybrid Reinforcement Learning (RL), where an agent learns from both an offline dataset and online explorations in an unknown environment, has garnered significant recent interest. A crucial question posed by Xie et al. (2022) is whether hybrid RL can improve upon the existing lower bounds established in purely offline and purely online RL without relying on the single-policy concentrability assumption. While Li et al. (2023) provided an affirmative answer to this question in the tabular PAC RL case, the question remains unsettled for both the regret-minimizing RL case and the non-tabular case. In this work, building upon recent advancements in offline RL and reward-agnostic exploration, we develop computationally efficient algorithms for both PAC and regret-minimizing RL with linear function approximation, without requiring concentrability on the entire state-action space. We demonstrate that these algorithms achieve sharper error or regret bounds that are no worse than, and can improve on, the optimal sample complexity in offline RL (the first algorithm, for PAC RL) and online RL (the second algorithm, for regret-minimizing RL) in linear Markov decision processes (MDPs), regardless of the quality of the behavior policy. To our knowledge, this work establishes the tightest theoretical guarantees currently available for hybrid RL in linear MDPs.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Kevin Tan",
        "Wei Fan",
        "Yuting Wei"
      ],
      "paper_id": "94483",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94483"
    },
    {
      "title": "Optimizing Automatic Differentiation with Deep Reinforcement Learning",
      "abstract": "Computing Jacobians with automatic differentiation is ubiquitous in many scientific domains such as machine learning, computational fluid dynamics, robotics and finance. Even small savings in the number of computations or memory usage in Jacobian computations can already incur massive savings in energy consumption and runtime. While there exist many methods that allow for such savings, they generally trade computational efficiency for approximations of the exact Jacobian.In this paper, we present a novel method to optimize the number of necessary multiplications for Jacobian computation by leveraging deep reinforcement learning (RL) and a concept called cross-country elimination while still computing the exact Jacobian. Cross-country elimination is a framework for automatic differentiation that phrases Jacobian accumulation as ordered elimination of all vertices on the computational graph where every elimination incurs a certain computational cost.Finding the optimal elimination order that minimizes the number of necessary multiplications can be seen as a single player game which in our case is played by an RL agent.We demonstrate that this method achieves up to 33% improvements over state-of-the-art methods on several relevant tasks taken from relevant domains.Furthermore, we show that these theoretical gains translate into actual runtime improvements by providing a cross-country elimination interpreter in JAX that can execute the obtained elimination orders.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Jamie Lohoff",
        "Emre Neftci"
      ],
      "paper_id": "94064",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94064"
    },
    {
      "title": "Towards Next-Generation Logic Synthesis: A Scalable Neural Circuit Generation Framework",
      "abstract": "Logic Synthesis (LS) aims to generate an optimized logic circuit satisfying a given functionality, which generally consists of circuit translation and optimization. It is a challenging and fundamental combinatorial optimization problem in integrated circuit design. Traditional LS approaches rely on manually designed heuristics to tackle the LS task, while machine learning recently offers a promising approach towards next-generation logic synthesis by neural circuit generation and optimization. In this paper, we first revisit the application of differentiable neural architecture search (DNAS) methods to circuit generation and found from extensive experiments that existing DNAS methods struggle to exactly generate circuits, scale poorly to large circuits, and exhibit high sensitivity to hyper-parameters. Then we provide three major insights for these challenges from extensive empirical analysis: 1) DNAS tends to overfit to too many skip-connections, consequently wasting a significant portion of the network's expressive capabilities; 2) DNAS suffers from the structure bias between the network architecture and the circuit inherent structure, leading to inefficient search; 3) the learning difficulty of different input-output examples varies significantly, leading to severely imbalanced learning. To address these challenges in a systematic way, we propose a novel regularized triangle-shaped circuit network generation framework, which leverages our key insights for completely accurate and scalable circuit generation. Furthermore, we propose an evolutionary algorithm assisted by reinforcement learning agent restarting technique for efficient and effective neural circuit optimization. Extensive experiments on four different circuit benchmarks demonstrate that our method can precisely generate circuits with up to 1200 nodes. Moreover, our synthesized circuits significantly outperform the state-of-the-art results from several competitive winners in IWLS 2022 and 2023 competitions.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Zhihai Wang",
        "Jie Wang",
        "Qingyue Yang",
        "Yinqi Bai",
        "Xing Li",
        "Lei Chen",
        "Jianye Hao",
        "Mingxuan Yuan",
        "Bin Li",
        "Yongdong Zhang",
        "Feng Wu"
      ],
      "paper_id": "94631",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94631"
    }
  ]
}