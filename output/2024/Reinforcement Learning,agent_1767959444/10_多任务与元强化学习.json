{
  "name": "多任务与元强化学习",
  "paper_count": 15,
  "summary": "该类别专注于多任务强化学习（MTRL）与元强化学习（Meta-RL）的研究。研究重点在于设计能够从多个相关任务中学习共享表示或元知识的算法，以实现对新任务的快速适应和泛化。相关工作旨在解决任务间的相似性与差异性平衡、表示学习、技能迁移以及高效探索等核心挑战，目标是开发出能够像人类一样灵活学习和适应广泛任务的通用智能体。",
  "papers": [
    {
      "title": "Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts",
      "abstract": "Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit similarities in terms of skills, objects, or physical properties while leveraging their representations eases the achievement of a universal policy. Nevertheless, the pursuit of learning a shared set of diverse representations is still an open challenge. In this paper, we introduce a novel approach for representation learning in MTRL that encapsulates common structures among the tasks using orthogonal representations to promote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts. When task-specific information is provided, MOORE generates relevant representations from this shared subspace. We assess the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses related baselines and establishes a new state-of-the-art result on MetaWorld.",
      "venue": "ICLR 2024",
      "authors": [
        "Ahmed Hendawy",
        "Jan Peters",
        "Carlo D&#x27;Eramo"
      ],
      "paper_id": "18365",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/18365"
    },
    {
      "title": "Provable Benefits of Multi-task RL under Non-Markovian Decision Making Processes",
      "abstract": "In multi-task reinforcement learning (RL) under Markov decision processes (MDPs), the presence of shared latent structures among multiple MDPs has been shown to yield significant benefits to the sample efficiency compared to single-task RL. In this paper, we investigate whether such a benefit can extend to more general sequential decision making problems such as predictive state representations (PSRs). The main challenge here is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce the model complexity and improve sample efficiency.To this end, we posit a  joint model class for tasks and use the notion of $\\eta$-bracketing number to quantify its complexity; this number also serves as a general metric  to capture the similarity of tasks and thus determines the benefit of multi-task over single-task RL. We first study  upstream multi-task learning over PSRs, in which all tasks share the same observation and action spaces. We propose a provably efficient algorithm  UMT-PSR for finding near-optimal policies for all PSRs, and demonstrate that the advantage of multi-task learning manifests if the joint model class of PSRs has a smaller $\\eta$-bracketing number compared to that of individual single-task learning. We further investigate downstream learning, in which the agent needs to learn a new target task that shares some commonalities with the upstream tasks via a similarity constraint. By exploiting the learned PSRs from the upstream, we develop a sample-efficient algorithm that provably finds a near-optimal policy. Upon specialization to some examples with small $\\eta$-bracketing numbers, our results further highlight the benefit compared to directly learning a single-task PSR.",
      "venue": "ICLR 2024",
      "authors": [
        "Ruiquan Huang",
        "Yuan Cheng",
        "Jing Yang",
        "Vincent Tan",
        "Yingbin Liang"
      ],
      "paper_id": "18535",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/18535"
    },
    {
      "title": "Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks",
      "abstract": "Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic  policy-sharing algorithm with myopic exploration design like $\\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the \"exploration benefits\" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control environments, where the diverse task set aligns with the task selection by automatic curriculum learning, which is empirically shown to improve sample-efficiency.",
      "venue": "ICLR 2024",
      "authors": [
        "Ziping Xu",
        "Zifan Xu",
        "Runxuan Jiang",
        "Peter Stone",
        "Ambuj Tewari"
      ],
      "paper_id": "18421",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/18421"
    },
    {
      "title": "DRED: Zero-Shot Transfer in Reinforcement Learning via Data-Regularised Environment Design",
      "abstract": "Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when these environments share characteristics with the ones they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which assume control over level generation. We find that existing UED methods can significantly shift the training distribution, which translates to low ZSG performance. To prevent both overfitting and distributional shift, we introduce *data-regularised environment design* (DRED). DRED generates levels using a generative model trained to approximate the ground truth distribution of an initial set of level parameters. Through its grounding, DRED achieves significant improvements in ZSG over adaptive level sampling strategies and UED methods.",
      "venue": "ICML 2024",
      "authors": [
        "Samuel Garcin",
        "James Doran",
        "Shangmin Guo",
        "Christopher Lucas",
        "Stefano V. Albrecht"
      ],
      "paper_id": "32810",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/32810"
    },
    {
      "title": "Meta-Reinforcement Learning Robust to Distributional Shift Via Performing Lifelong In-Context Learning",
      "abstract": "A key challenge in Meta-Reinforcement Learning (meta-RL) is the task distribution shift, since the generalization ability of most current meta-RL methods is limited to tasks sampled from the training distribution. In this paper, we propose Posterior Sampling Bayesian Lifelong In-Context Reinforcement Learning (PSBL), which is robust to task distribution shift. PSBL meta-trains a variant of transformer to directly perform amortized inference about the Predictive Posterior Distribution (PPD) of the optimal policy. Once trained, the network can infer the PPD online with frozen parameters. The agent then samples actions from the approximate PPD to perform online exploration, which progressively reduces uncertainty and enhances performance in the interaction with the environment. This property is known as in-context learning. Experimental results demonstrate that PSBL significantly outperforms standard Meta RL methods both in tasks with sparse rewards and dense rewards when the test task distribution is strictly shifted from the training distribution.",
      "venue": "ICML 2024",
      "authors": [
        "TengYe Xu",
        "Zihao Li",
        "Qinyuan Ren"
      ],
      "paper_id": "33209",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/33209"
    },
    {
      "title": "Test-Time Regret Minimization in Meta Reinforcement Learning",
      "abstract": "Meta reinforcement learning sets a distribution over a set of tasks on which the agent can train at will, then is asked to learn an optimal policy for any test task efficiently. In this paper, we consider a finite set of tasks modeled through Markov decision processes with various dynamics. We assume to have endured a long training phase, from which the set of tasks is perfectly recovered, and we focus on regret minimization against the optimal policy in the unknown test task. Under a separation condition that states the existence of a state-action pair revealing a task against another, Chen et al. (2022) show that $O(M^2 \\log(H))$ regret can be achieved, where $M, H$ are the number of tasks in the set and test episodes, respectively. In our first contribution, we demonstrate that the latter rate is nearly optimal by developing a novel lower bound for test-time regret minimization under separation, showing that a linear dependence with $M$ is unavoidable. Then, we present a family of stronger yet reasonable assumptions beyond separation, which we call strong identifiability, enabling algorithms achieving fast rates $\\log (H)$ and sublinear dependence with $M$ simultaneously. Our paper provides a new understanding of the statistical barriers of test-time regret minimization and when fast rates can be achieved.",
      "venue": "ICML 2024",
      "authors": [
        "Mirco Mutti",
        "Aviv Tamar"
      ],
      "paper_id": "34298",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34298"
    },
    {
      "title": "AMAGO-2: Breaking the Multi-Task Barrier in Meta-Reinforcement Learning with Transformers",
      "abstract": "Language models trained on diverse datasets unlock generalization by in-context learning. Reinforcement Learning (RL) policies can achieve a similar effect by meta-learning within the memory of a sequence model. However, meta-RL research primarily focuses on adapting to minor variations of a single task. It is difficult to scale towards more general behavior without confronting challenges in multi-task optimization, and few solutions are compatible with meta-RL's goal of learning from large training sets of unlabeled tasks. To address this challenge, we revisit the idea that multi-task RL is bottlenecked by imbalanced training losses created by uneven return scales across different tasks. We build upon recent advancements in Transformer-based (in-context) meta-RL and evaluate a simple yet scalable solution where both an agent's actor and critic objectives are converted to classification terms that decouple optimization from the current scale of returns. Large-scale comparisons in Meta-World ML45, Multi-Game Procgen, Multi-Task POPGym, Multi-Game Atari, and BabyAI find that this design unlocks significant progress in online multi-task adaptation and memory problems without explicit task labels.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Jake Grigsby",
        "Justin Sasek",
        "Samyak Parajuli",
        "Ikechukwu D. Adebi",
        "Amy Zhang",
        "Yuke Zhu"
      ],
      "paper_id": "95369",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/95369"
    },
    {
      "title": "Balancing Context Length and Mixing Times for Reinforcement Learning at Scale",
      "abstract": "Due to the recent remarkable advances in artificial intelligence, researchers have begun to consider challenging learning problems such as learning to generalize behavior from large offline datasets or learning online in non-Markovian environments. Meanwhile, recent advances in both of these areas have increasingly relied on conditioning policies on large context lengths. A natural question is if there is a limit to the performance benefits of increasing the context length if the computation needed is available. In this work, we establish a novel theoretical result that links the context length of a policy to the time needed to reliably evaluate its performance (i.e., its mixing time) in large scale partially observable reinforcement learning environments that exhibit latent sub-task structure. This analysis underscores a key tradeoff: when we extend the context length, our policy can more effectively model non-Markovian dependencies, but this comes at the cost of potentially slower policy evaluation and as a result slower downstream learning. Moreover, our empirical results highlight the relevance of this analysis when leveraging Transformer based neural networks. This perspective will become increasingly pertinent as the field scales towards larger and more realistic environments, opening up a number of potential future directions for improving the way we design learning agents.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Matthew Riemer",
        "Khimya Khetarpal",
        "Janarthanan Rajendran",
        "Sarath Chandar"
      ],
      "paper_id": "94886",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94886"
    },
    {
      "title": "Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting Diversity",
      "abstract": "The wider application of end-to-end learning methods to embodied decision-making domains remains bottlenecked by their reliance on a superabundance of training data representative of the target domain.Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot *generalization*—the goal of standard reinforcement learning (RL)—in favor of few-shot *adaptation*, and thus hold promise for bridging larger generalization gaps.While learning this meta-level adaptive behavior still requires substantial data, efficient environment simulators approaching real-world complexity are growing in prevalence.Even so, hand-designing sufficiently diverse and numerous simulated training tasks for these complex domains is prohibitively labor-intensive.Domain randomization (DR) and procedural generation (PG), offered as solutions to this problem, require simulators to possess carefully-defined parameters which directly translate to meaningful task diversity—a similarly prohibitive assumption.In this work, we present **DIVA**, an evolutionary approach for generating diverse training tasks in such complex, open-ended simulators.Like unsupervised environment design (UED) methods, DIVA can be applied to arbitrary parameterizations, but can additionally incorporate realistically-available domain knowledge—thus inheriting the *flexibility* and *generality* of UED, and the supervised *structure* embedded in well-designed simulators exploited by DR and PG.Our empirical results showcase DIVA's unique ability to overcome complex parameterizations and successfully train adaptive agent behavior, far outperforming competitive baselines from prior literature.These findings highlight the potential of such *semi-supervised environment design* (SSED) approaches, of which DIVA is the first humble constituent, to enable training in realistic simulated domains, and produce more robust and capable adaptive agents.Our code is available at [https://github.com/robbycostales/diva](https://github.com/robbycostales/diva).",
      "venue": "NeurIPS 2024",
      "authors": [
        "Robby Costales",
        "Stefanos Nikolaidis"
      ],
      "paper_id": "94744",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94744"
    },
    {
      "title": "First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs",
      "abstract": "Standard reinforcement learning (RL) agents never intelligently explore like a human (i.e. taking into account complex domain priors and adapting quickly based on previous exploration). Across episodes, RL agents struggle to perform even simple exploration strategies, for example systematic search that avoids exploring the same location multiple times. This poor exploration limits performance on challenging domains. Meta-RL is a potential solution, as unlike standard RL, meta-RL can *learn* to explore, and potentially learn highly complex strategies far beyond those of standard RL, strategies such as experimenting in early episodes to learn new skills, or conducting experiments to learn about the current environment.Traditional meta-RL focuses on the problem of learning to optimally balance exploration and exploitation to maximize the *cumulative reward* of the episode sequence (e.g., aiming to maximize the total wins in a tournament -- while also improving as a player).We identify a new challenge with state-of-the-art cumulative-reward meta-RL methods.When optimal behavior requires exploration that sacrifices immediate reward to enable higher subsequent reward, existing state-of-the-art cumulative-reward meta-RL methods become stuck on the local optimum of failing to explore.Our method, First-Explore, overcomes this limitation by learning two policies: one to solely explore, and one to solely exploit. When exploring requires forgoing early-episode reward, First-Explore significantly outperforms existing cumulative meta-RL methods. By identifying and solving the previously unrecognized problem of forgoing reward in early episodes, First-Explore represents a significant step towards developing meta-RL algorithms capable of human-like exploration on a broader range of domains.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Ben Norman",
        "Jeff Clune"
      ],
      "paper_id": "96238",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/96238"
    },
    {
      "title": "Meta-DT: Offline Meta-RL as Conditional Sequence Modeling with World Model Disentanglement",
      "abstract": "A longstanding goal of artificial general intelligence is highly capable generalists that can learn from diverse experiences and generalize to unseen tasks. The language and vision communities have seen remarkable progress toward this trend by scaling up transformer-based models trained on massive datasets, while reinforcement learning (RL) agents still suffer from poor generalization capacity under such paradigms. To tackle this challenge, we propose Meta Decision Transformer (Meta-DT), which leverages the sequential modeling ability of the transformer architecture and robust task representation learning via world model disentanglement to achieve efficient generalization in offline meta-RL. We pretrain a context-aware world model to learn a compact task representation, and inject it as a contextual condition to the causal transformer to guide task-oriented sequence generation. Then, we subtly utilize history trajectories generated by the meta-policy as a self-guided prompt to exploit the architectural inductive bias. We select the trajectory segment that yields the largest prediction error on the pretrained world model to construct the prompt, aiming to encode task-specific information complementary to the world model maximally. Notably, the proposed framework eliminates the requirement of any expert demonstration or domain knowledge at test time. Experimental results on MuJoCo and Meta-World benchmarks across various dataset types show that Meta-DT exhibits superior few and zero-shot generalization capacity compared to strong baselines while being more practical with fewer prerequisites. Our code is available at https://github.com/NJU-RL/Meta-DT.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Zhi Wang",
        "Li Zhang",
        "Wenhao Wu",
        "Yuanheng Zhu",
        "Dongbin Zhao",
        "Chunlin Chen"
      ],
      "paper_id": "94988",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94988"
    },
    {
      "title": "Offline Multitask Representation Learning for Reinforcement Learning",
      "abstract": "We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Haque Ishfaq",
        "Thanh Nguyen-Tang",
        "Songtao Feng",
        "Raman Arora",
        "Mengdi Wang",
        "Ming Yin",
        "Doina Precup"
      ],
      "paper_id": "96488",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/96488"
    },
    {
      "title": "Parseval Regularization for Continual Reinforcement Learning",
      "abstract": "Plasticity loss, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---referring to the increased difficulty in training on new tasks.We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting.We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks.We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Wesley Chung",
        "Lynn Cherif",
        "Doina Precup",
        "David Meger"
      ],
      "paper_id": "95192",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/95192"
    },
    {
      "title": "PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning",
      "abstract": "Designing generalizable agents capable of adapting to diverse embodiments has achieved significant attention in Reinforcement Learning (RL), which is critical for deploying RL agents in various real-world applications. Previous Cross-Embodiment RL approaches have focused on transferring knowledge across embodiments within specific tasks. These methods often result in knowledge tightly coupled with those tasks and fail to adequately capture the distinct characteristics of different embodiments. To address this limitation, we introduce the notion of Cross-Embodiment Unsupervised RL (CEURL), which leverages unsupervised learning to enable agents to acquire embodiment-aware and task-agnostic knowledge through online interactions within reward-free environments. We formulate CEURL as a novel Controlled Embodiment Markov Decision Process (CE-MDP) and systematically analyze CEURL's pre-training objectives under CE-MDP. Based on these analyses, we develop a novel algorithm Pre-trained Embodiment-Aware Control (PEAC) for handling CEURL, incorporating an intrinsic reward function specifically designed for cross-embodiment pre-training. PEAC not only provides an intuitive optimization strategy for cross-embodiment pre-training but also can integrate flexibly with existing unsupervised RL methods, facilitating cross-embodiment exploration and skill discovery. Extensive experiments in both simulated (e.g., DMC and Robosuite) and real-world environments (e.g., legged locomotion) demonstrate that PEAC significantly improves adaptation performance and cross-embodiment generalization, demonstrating its effectiveness in overcoming the unique challenges of CEURL. The project page and code are in https://yingchengyang.github.io/ceurl.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Chengyang Ying",
        "Hao Zhongkai",
        "Xinning Zhou",
        "Xuezhou Xu",
        "Hang Su",
        "Xingxing Zhang",
        "Jun Zhu"
      ],
      "paper_id": "95555",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/95555"
    },
    {
      "title": "RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space",
      "abstract": "Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies. Existing works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return.In this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy. This enables us to recast the DT extraction problem into a novel non-euclidean clustering problem over the local observation and action values space of each agent, with action values as cluster labels and the upper bound on the return gap as clustering loss.Both the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents. Further, we propose the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a surprisingly simple design and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss. Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes).",
      "venue": "NeurIPS 2024",
      "authors": [
        "Jingdi Chen",
        "Hanhan Zhou",
        "Yongsheng Mei",
        "Carlee Joe-Wong",
        "Gina C. Adam",
        "Nathaniel Bastian",
        "Tian Lan"
      ],
      "paper_id": "93745",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/93745"
    }
  ]
}