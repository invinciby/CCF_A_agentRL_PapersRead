{
  "name": "强化学习系统与平台",
  "paper_count": 9,
  "summary": "该类别专注于强化学习系统、平台和工程化实现的研究。研究重点在于开发高效、可扩展、可复现的分布式或单机强化学习训练框架与基础设施。相关工作旨在解决大规模RL训练中的计算效率、资源利用、代码可维护性和实验结果可复现性等实际问题，为算法研究提供稳定可靠的实验和部署基础。例如，通过优化actor-learner架构、设计新的并行化方案或构建开源平台来提升训练速度与稳定性。",
  "papers": [
    {
      "title": "Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform",
      "abstract": "Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner framework can have reproducibility issues even if hyperparameters are controlled. We then introduce Cleanba, a new open-source platform for distributed DRL that proposes a highly reproducible architecture. Cleanba implements highly optimized distributed variants of PPO and IMPALA. Our Atari experiments show that these variants can obtain equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL. However, Cleanba variants present 1) shorter training time and 2) more reproducible learning curves in different hardware settings.",
      "venue": "ICLR 2024",
      "authors": [
        "Shengyi Huang",
        "Jiayi Weng",
        "Rujikorn Charakorn",
        "Min Lin",
        "Zhongwen Xu",
        "Santiago Ontanon"
      ],
      "paper_id": "19132",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/19132"
    },
    {
      "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX",
      "abstract": "Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms.In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications.Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable.Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks.By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents.Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs.Furthermore, we provide actor-critic baselines for each environment, accompanied by preliminary findings on scaling and generalization scenarios.Jumanji aims to set a new standard for speed, adaptability, and scalability of RL environments.",
      "venue": "ICLR 2024",
      "authors": [
        "Clem Bonnet",
        "Daniel Luo",
        "Donal Byrne",
        "Shikha Surana",
        "Sasha Abramowitz",
        "Paul Duckworth",
        "Vincent Coyette",
        "Laurence Midgley",
        "Elshadai Tegegn",
        "Tristan Kalloniatis",
        "Omayma Mahjoub",
        "Matthew Macfarlane",
        "Andries Smit",
        "Nathan Grinsztajn",
        "Raphael Boige",
        "Cemlyn Waters",
        "Mohamed Ali Mimouni",
        "Ulrich Mbou Sob",
        "Ruan de Kock",
        "Siddarth Singh",
        "Daniel Furelos-Blanco",
        "Victor Le",
        "Arnu Pretorius",
        "Alexandre Laterre"
      ],
      "paper_id": "19187",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/19187"
    },
    {
      "title": "Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments",
      "abstract": "We explore a Federated Reinforcement Learning (FRL) problem where $N$ agents collaboratively learn a common policy without sharing their trajectory data. To date, existing FRL work has primarily focused on agents operating in the same or ``similar\" environments. In contrast, our problem setup allows for arbitrarily large levels of environment heterogeneity. To obtain the optimal policy which maximizes the average performance across all *potentially completely different* environments, we propose two algorithms: FedSVRPG-M and FedHAPG-M. In contrast to existing results, we demonstrate that both FedSVRPG-M and FedHAPG-M, both of which leverage momentum mechanisms, can exactly converge to a stationary point of the average performance function, regardless of the magnitude of environment heterogeneity. Furthermore, by incorporating the benefits of variance-reduction techniques or Hessian approximation, both algorithms achieve state-of-the-art convergence results, characterized by a sample complexity of $\\mathcal{O}\\left(\\epsilon^{-\\frac{3}{2}}/N\\right)$. Notably, our algorithms enjoy linear convergence speedups with respect to the number of agents, highlighting the benefit of collaboration among agents in finding a common policy.",
      "venue": "ICML 2024",
      "authors": [
        "Han Wang",
        "Sihong He",
        "Zhili Zhang",
        "Fei Miao",
        "James Anderson"
      ],
      "paper_id": "33442",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/33442"
    },
    {
      "title": "BricksRL: A Platform for Democratizing Robotics and  Reinforcement Learning Research and Education with LEGO",
      "abstract": "We present BricksRL, a platform designed to democratize access to robotics for reinforcement learning research and education. BricksRL facilitates the creation, design, and training of custom LEGO robots in the real world by interfacing them with the TorchRL library for reinforcement learning agents. The integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional communication, enables state-of-the-art reinforcement learning training on GPUs for a wide variety of LEGO builds. This offers a flexible and cost-efficient approach for scaling and also provides a robust infrastructure for robot-environment-algorithm communication. We present various experiments across tasks and robot configurations, providing built plans and training results. Furthermore, we demonstrate that inexpensive LEGO robots can be trained end-to-end in the real world to achieve simple tasks, with training times typically under 120 minutes on a normal laptop. Moreover, we show how users can extend the capabilities, exemplified by the successful integration of non-LEGO sensors. By enhancing accessibility to both robotics and reinforcement learning, BricksRL establishes a strong foundation for democratized robotic learning in research and educational settings.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Sebastian Dittert",
        "Vincent Moens",
        "Gianni De Fabritiis"
      ],
      "paper_id": "96358",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/96358"
    },
    {
      "title": "JaxMARL: Multi-Agent RL Environments and Algorithms in JAX",
      "abstract": "Benchmarks are crucial in the development of machine learning algorithms, significantly influencing reinforcement learning (RL) research through the available environments. Traditionally, RL environments run on the CPU, which limits their scalability with the computational resources typically available in academia. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, easy-to-use code base that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is up to 12,500 times faster than existing approaches. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. The code is available at https://github.com/flairox/jaxmarl.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Alexander Rutherford",
        "Benjamin Ellis",
        "Matteo Gallici",
        "Jonathan Cook",
        "Andrei Lupu",
        "Garðar Ingvarsson Juto",
        "Timon Willi",
        "Ravi Hammond",
        "Akbir Khan",
        "Christian Schroeder de Witt",
        "Alexandra Souly",
        "Saptarashmi Bandyopadhyay",
        "Mikayel Samvelyan",
        "Minqi Jiang",
        "Robert Lange",
        "Shimon Whiteson",
        "Bruno Lacerda",
        "Nick Hawes",
        "Tim Rocktäschel",
        "Chris Lu",
        "Jakob Foerster"
      ],
      "paper_id": "97649",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/97649"
    },
    {
      "title": "ODRL: A Benchmark for Off-Dynamics Reinforcement Learning",
      "abstract": "We consider off-dynamics reinforcement learning (RL) where one needs to transfer policies across different domains with dynamics mismatch. Despite the focus on developing dynamics-aware algorithms, this field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ODRL, the first benchmark tailored for evaluating off-dynamics RL methods. ODRL contains four experimental settings where the source and target domains can be either online or offline, and provides diverse tasks and a broad spectrum of dynamics shifts, making it a reliable platform to comprehensively evaluate the agent's adaptation ability to the target domain. Furthermore, ODRL includes recent off-dynamics RL algorithms in a unified framework and introduces some extra baselines for different settings, all implemented in a single-file manner. To unpack the true adaptation capability of existing methods, we conduct extensive benchmarking experiments, which show that no method has universal advantages across varied dynamics shifts. We hope this benchmark can serve as a cornerstone for future research endeavors. Our code is publicly available at https://github.com/OffDynamicsRL/off-dynamics-rl.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Jiafei Lyu",
        "Kang Xu",
        "Jiacheng Xu",
        "yan",
        "Jing-Wen Yang",
        "Zongzhang Zhang",
        "Chenjia Bai",
        "Zongqing Lu",
        "Xiu Li"
      ],
      "paper_id": "97619",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/97619"
    },
    {
      "title": "PUZZLES: A Benchmark for Neural Algorithmic Reasoning",
      "abstract": "Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments. These advancements have been enabled in part by the availability of benchmarks. In this work we introduce PUZZLES, a benchmark based on Simon Tatham's Portable Puzzle Collection, aimed at fostering progress in algorithmic and logical reasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizes and varying levels of complexity, providing detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we evaluate various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research. All the software, including the environment, is available at this https url.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Benjamin Estermann",
        "Luca Lanzendörfer",
        "Yannick Niedermayr",
        "Roger Wattenhofer"
      ],
      "paper_id": "97818",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/97818"
    },
    {
      "title": "Using Unity to Help Solve Reinforcement Learning",
      "abstract": "Leveraging the depth and flexibility of XLand as well as the rapid prototyping features of the Unity engine, we present the United Unity Universe — an open-source toolkit designed to accelerate the creation of innovative reinforcement learning environments. This toolkit includes a robust implementation of XLand 2.0 complemented by a user-friendly interface which allows users to modify the details of procedurally generated terrains and task rules with ease. Additionally, we provide a curated selection of terrains and rule sets, accompanied by implementations of reinforcement learning baselines to facilitate quick experimentation with novel architectural designs for adaptive agents. Furthermore, we illustrate how the United Unity Universe serves as a high-level language that enables researchers to develop diverse and endlessly variable 3D environments within a unified framework. This functionality establishes the United Unity Universe (U3) as an essential tool for advancing the field of reinforcement learning, especially in the development of adaptive and generalizable learning systems.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Connor Brennan",
        "Andrew Williams",
        "Omar G. Younis",
        "Vedant Vyas",
        "Daria Yasafova",
        "Irina Rish"
      ],
      "paper_id": "97723",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/97723"
    },
    {
      "title": "XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX",
      "abstract": "Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at \\url{https://github.com/corl-team/xland-minigrid}.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Alexander Nikulin",
        "Vladislav Kurenkov",
        "Ilya Zisman",
        "Artem Agarkov",
        "Viacheslav Sinii",
        "Sergey Kolesnikov"
      ],
      "paper_id": "97425",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/97425"
    }
  ]
}