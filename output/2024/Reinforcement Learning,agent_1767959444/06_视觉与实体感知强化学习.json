{
  "name": "视觉与实体感知强化学习",
  "paper_count": 14,
  "summary": "该类别专注于从高维视觉输入（如图像或像素）中学习控制策略的强化学习方法，特别是涉及对环境中多个实体（如物体）进行感知、推理和操作的任务。研究重点在于设计具有结构化表示的算法，以应对视觉RL中的样本效率低、维度灾难以及泛化能力差等挑战。相关工作旨在使智能体能够理解场景中的实体及其关系，并基于此进行目标驱动的决策，最终实现从像素到动作的端到端学习，并具备良好的组合泛化能力。",
  "papers": [
    {
      "title": "Entity-Centric Reinforcement Learning for Object Manipulation from Pixels",
      "abstract": "Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: https://sites.google.com/view/entity-centric-rl",
      "venue": "ICLR 2024",
      "authors": [
        "Dan Haramati",
        "Tal Daniel",
        "Aviv Tamar"
      ],
      "paper_id": "17585",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/17585"
    },
    {
      "title": "Learning Interactive Real-World Simulators",
      "abstract": "Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by x,y” from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.",
      "venue": "ICLR 2024",
      "authors": [
        "Sherry Yang",
        "Yilun Du",
        "Seyed Ghasemipour",
        "Jonathan Tompson",
        "Leslie Kaelbling",
        "Dale Schuurmans",
        "Pieter Abbeel"
      ],
      "paper_id": "17660",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/17660"
    },
    {
      "title": "Privileged Sensing Scaffolds Reinforcement Learning",
      "abstract": "We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon “sensory scaffolding”: observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose “Scaffolder”, a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new “S3” suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/",
      "venue": "ICLR 2024",
      "authors": [
        "Edward Hu",
        "James Springer",
        "Oleh Rybkin",
        "Dinesh Jayaraman"
      ],
      "paper_id": "19090",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/19090"
    },
    {
      "title": "Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages",
      "abstract": "Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased reuse frequency. Rather than setting a static RR for the entire training process, we propose Adaptive RR, which dynamically adjusts the RR based on the critic’s plasticity level. Extensive evaluations indicate that Adaptive RR not only avoids catastrophic plasticity loss in the early stages but also benefits from more frequent reuse in later phases, resulting in superior sample efficiency.",
      "venue": "ICLR 2024",
      "authors": [
        "Guozheng Ma",
        "Lu Li",
        "Sen Zhang",
        "Zixuan Liu",
        "Zhen Wang",
        "Yixin Chen",
        "Li Shen",
        "Xueqian Wang",
        "Dacheng Tao"
      ],
      "paper_id": "19614",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/19614"
    },
    {
      "title": "Towards Principled Representation Learning from Videos for Reinforcement Learning",
      "abstract": "We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for representation learning and focus on learning the latent state representations of the underlying MDP using video data. We study two types of settings: one where there is iid noise in the observation, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background. We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling. We prove upper bounds for temporal contrastive learning and forward modeling in the presence of only iid noise. We show that these approaches can learn the latent state and use it to do efficient downstream RL with polynomial sample complexity. When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data. This partially explains why reinforcement learning with video pre-training is hard. We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings.",
      "venue": "ICLR 2024",
      "authors": [
        "Dipendra Kumar Misra",
        "Akanksha Saran",
        "Tengyang Xie",
        "Alex Lamb",
        "John Langford"
      ],
      "paper_id": "19497",
      "pdf_url": "",
      "forum_url": "https://iclr.cc/forum?id=2024/poster/19497"
    },
    {
      "title": "BeigeMaps: Behavioral Eigenmaps for Reinforcement Learning from Images",
      "abstract": "Training reinforcement learning (RL) agents directly from high-dimensional image observations continues to be a challenging problem. Recent line of work on behavioral distances proposes to learn representations that encode behavioral similarities quantified by the bisimulation metric. By learning an isometric mapping to a lower dimensional space that preserves this metric, such methods attempt to learn representations that group together functionally similar states. However, such an isometric mapping may not exist, making the learning objective ill-defined. We propose an alternative objective that allows distortions in long-range distances, while preserving *local* metric structure -- inducing representations that highlight natural clusters in the state space. This leads to new representations, which we term Behavioral Eigenmaps (BeigeMaps), corresponding to the eigenfunctions of similarity kernels induced by behavioral distances. We empirically demonstrate that when added as a drop-in modification, BeigeMaps improve the policy performance of prior behavioral distance based RL algorithms.",
      "venue": "ICML 2024",
      "authors": [
        "Sandesh Adhikary",
        "Anqi Li",
        "Byron Boots"
      ],
      "paper_id": "33147",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/33147"
    },
    {
      "title": "Investigating Pre-Training Objectives for Generalization in Vision-Based Reinforcement Learning",
      "abstract": "Recently, various pre-training methods have been introduced in vision-based Reinforcement Learning (RL). However, their generalization ability remains unclear due to evaluations being limited to in-distribution environments and non-unified experimental setups. To address this, we introduce the Atari Pre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10 million transitions from 50 Atari games and evaluates it across diverse environment distributions. Our experiments show that pre-training objectives focused on learning task-agnostic features (e.g., identifying objects and understanding temporal dynamics) enhance generalization across different environments. In contrast, objectives focused on learning task-specific knowledge (e.g., identifying agents and fitting reward functions) improve performance in environments similar to the pre-training dataset but not in varied ones. We publicize our codes, datasets, and model checkpoints at https://github.com/dojeon-ai/Atari-PB.",
      "venue": "ICML 2024",
      "authors": [
        "Donghu Kim",
        "Hojoon Lee",
        "Kyungmin Lee",
        "Dongyoon Hwang",
        "Jaegul Choo"
      ],
      "paper_id": "34150",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34150"
    },
    {
      "title": "Prompt-based Visual Alignment for Zero-shot Policy Transfer",
      "abstract": "Overfitting in RL has become one of the main obstacles to applications in reinforcement learning(RL). Existing methods do not provide explicit semantic constrain for the feature extractor, hindering the agent from learning a unified cross-domain representation and resulting in performance degradation on unseen domains. Besides, abundant data from multiple domains are needed. To address these issues, in this work, we propose prompt-based visual alignment (PVA), a robust framework to mitigate the detrimental domain bias in the image for zero-shot policy transfer. Inspired that Visual-Language Model (VLM) can serve as a bridge to connect both text space and image space, we leverage the semantic information contained in a text sequence as an explicit constraint to train a visual aligner. Thus, the visual aligner can map images from multiple domains to a unified domain and achieve good generalization performance. To better depict semantic information, prompt tuning is applied to learn a sequence of learnable tokens. With explicit constraints of semantic information, PVA can learn unified cross-domain representation under limited access to cross-domain data and achieves great zero-shot generalization ability in unseen domains. We verify PVA on a vision-based autonomous driving task with CARLA simulator. Experiments show that the agent generalizes well on unseen domains under limited access to multi-domain data.",
      "venue": "ICML 2024",
      "authors": [
        "Haihan Gao",
        "Rui Zhang",
        "Qi Yi",
        "Hantao Yao",
        "Haochen Li",
        "Jiaming Guo",
        "Shaohui Peng",
        "Yunkai Gao",
        "QiCheng Wang",
        "Xing Hu",
        "Yuanbo Wen",
        "Zihao Zhang",
        "Zidong Du",
        "Ling Li",
        "Qi Guo",
        "Yunji Chen"
      ],
      "paper_id": "34124",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34124"
    },
    {
      "title": "Rich-Observation Reinforcement Learning with Continuous Latent Dynamics",
      "abstract": "Sample-efficiency and reliability remain major bottlenecks toward wide adoption of reinforcement learning algorithms in continuous settings with high-dimensional perceptual inputs. Toward addressing these challenges, we introduce a new theoretical framework, **RichCLD** (“Rich-Observation RL with Continuous Latent Dynamics”), in which the agent performs control based on high-dimensional observations, but the environment is governed by low-dimensional latent states and Lipschitz continuous dynamics. Our main contribution is a new algorithm for this setting that is provably statistically and computationally efficient. The core of our algorithm is a new representation learning objective; we show that prior representation learning schemes tailored to discrete dynamics do not naturally extend to the continuous setting. Our new objective is amenable to practical implementation, and empirically, we find that it compares favorably to prior schemes in a standard evaluation protocol. We further provide several insights into the statistical complexity of the **RichCLD** framework, in particular proving that certain notions of Lipschitzness that admit sample-efficient learning in the absence of rich observations are insufficient in the rich-observation setting.",
      "venue": "ICML 2024",
      "authors": [
        "Yuda Song",
        "Lili Wu",
        "Dylan Foster",
        "Akshay Krishnamurthy"
      ],
      "paper_id": "34487",
      "pdf_url": "",
      "forum_url": "https://ICML.cc/forum?id=2024/poster/34487"
    },
    {
      "title": "A Simple Framework for Generalization in Visual RL under Dynamic Scene Perturbations",
      "abstract": "In the rapidly evolving domain of vision-based deep reinforcement learning (RL), a pivotal challenge is to achieve generalization capability to dynamic environmental changes reflected in visual observations.Our work delves into the intricacies of this problem, identifying two key issues that appear in previous approaches for visual RL generalization: (i) imbalanced saliency and (ii) observational overfitting.Imbalanced saliency is a phenomenon where an RL agent disproportionately identifies salient features across consecutive frames in a frame stack. Observational overfitting occurs when the agent focuses on certain background regions rather than task-relevant objects.To address these challenges, we present a simple yet effective framework for generalization in visual RL (SimGRL) under dynamic scene perturbations.First, to mitigate the imbalanced saliency problem, we introduce an architectural modification to the image encoder to stack frames at the feature level rather than the image level.Simultaneously, to alleviate the observational overfitting problem, we propose a novel technique called shifted random overlay augmentation, which is specifically designed to learn robust representations capable of effectively handling dynamic visual scenes.Extensive experiments demonstrate the superior generalization capability of SimGRL, achieving state-of-the-art performance in benchmarks including the DeepMind Control Suite.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Wonil Song",
        "Hyesong Choi",
        "Kwanghoon Sohn",
        "Dongbo Min"
      ],
      "paper_id": "96947",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/96947"
    },
    {
      "title": "DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors",
      "abstract": "Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint.  In this paper, we present theDeepMind Control Visual Benchmark (DMC-VB), a dataset collected in the DeepMind Control Suite to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at https://github.com/google-deepmind/dmc_vision_benchmark.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Joseph Ortiz",
        "Antoine Dedieu",
        "Wolfgang Lehrach",
        "J Swaroop Guntupalli",
        "Carter Wendelken",
        "Ahmad Humayun",
        "Sivaramakrishnan Swaminathan",
        "Guangyao Zhou",
        "Miguel Lazaro-Gredilla",
        "Kevin Murphy"
      ],
      "paper_id": "97877",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/97877"
    },
    {
      "title": "Focus On What Matters: Separated Models For Visual-Based RL Generalization",
      "abstract": "A primary challenge for visual-based Reinforcement Learning (RL) is to generalize effectively across unseen environments. Although previous studies have explored different auxiliary tasks to enhance generalization, few adopt image reconstruction due to concerns about exacerbating overfitting to task-irrelevant features during training. Perceiving the pre-eminence of image reconstruction in representation learning, we propose SMG (\\blue{S}eparated \\blue{M}odels for \\blue{G}eneralization), a novel approach that exploits image reconstruction for generalization. SMG introduces two model branches to extract task-relevant and task-irrelevant representations separately from visual observations via cooperatively reconstruction. Built upon this architecture, we further emphasize the importance of task-relevant features for generalization. Specifically, SMG incorporates two additional consistency losses to guide the agent's focus toward task-relevant areas across different scenarios, thereby achieving free from overfitting. Extensive experiments in DMC demonstrate the SOTA performance of SMG in generalization, particularly excelling in video-background settings. Evaluations on robotic manipulation tasks further confirm the robustness of SMG in real-world applications. Source code is available at \\url{https://anonymous.4open.science/r/SMG/}.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Di Zhang",
        "Bowen Lv",
        "Hai Zhang",
        "Feifan Yang",
        "Junqiao Zhao",
        "Hang Yu",
        "Chang Huang",
        "Hongtu Zhou",
        "Chen Ye",
        "changjun jiang"
      ],
      "paper_id": "93111",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/93111"
    },
    {
      "title": "GOMAA-Geo: GOal Modality Agnostic Active Geo-localization",
      "abstract": "We consider the task of active geo-localization (AGL) in which an agent uses a sequence of visual cues observed during aerial navigation to find a target specified through multiple possible modalities. This could emulate a UAV involved in a search-and-rescue operation navigating through an area, observing a stream of aerial images as it goes. The AGL task is associated with two important challenges.  Firstly, an agent must deal with a goal specification in one of multiple modalities (e.g., through a natural language description) while the search cues are provided in other modalities (aerial imagery). The second challenge is limited localization time (e.g., limited battery life, urgency) so that the goal must be localized as efficiently as possible, i.e. the agent must effectively leverage its sequentially observed aerial views when searching for the goal. To address these challenges, we propose GOMAA-Geo -- a goal modality agnostic active geo-localization agent -- for zero-shot generalization between different goal modalities. Our approach combines cross-modality contrastive learning to align representations across modalities with supervised foundation model pretraining and reinforcement learning to obtain highly effective navigation and localization policies. Through extensive evaluations, we show that GOMAA-Geo outperforms alternative learnable approaches and that it generalizes across datasets -- e.g., to disaster-hit areas without seeing a single disaster scenario during training -- and goal modalities -- e.g., to ground-level imagery or textual descriptions, despite only being trained with goals specified as aerial views. Our code is available at: https://github.com/mvrl/GOMAA-Geo.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Anindya Sarkar",
        "Srikumar Sastry",
        "Aleksis Pirinen",
        "Chongjie Zhang",
        "Nathan Jacobs",
        "Yevgeniy Vorobeychik"
      ],
      "paper_id": "94144",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/94144"
    },
    {
      "title": "The Surprising Ineffectiveness of Pre-Trained Visual Representations for Model-Based Reinforcement Learning",
      "abstract": "Visual Reinforcement Learning (RL) methods often require extensive amounts of data. As opposed to model-free RL, model-based RL (MBRL) offers a potential solution with efficient data utilization through planning. Additionally, RL lacks generalization capabilities for real-world tasks. Prior work has shown that incorporating pre-trained visual representations (PVRs) enhances sample efficiency and generalization. While PVRs have been extensively studied in the context of model-free RL, their potential in MBRL remains largely unexplored. In this paper, we benchmark a set of PVRs on challenging control tasks in a model-based RL setting. We investigate the data efficiency, generalization capabilities, and the impact of different properties of PVRs on the performance of model-based agents. Our results, perhaps surprisingly, reveal that for MBRL current PVRs are not more sample efficient than learning representations from scratch, and that they do not generalize better to out-of-distribution (OOD) settings. To explain this, we analyze the quality of the trained dynamics model. Furthermore, we show that data diversity and network architecture are the most important contributors to OOD generalization performance.",
      "venue": "NeurIPS 2024",
      "authors": [
        "Moritz Schneider",
        "Robert Krug",
        "Narunas Vaskevicius",
        "Luigi Palmieri",
        "Joschka Boedecker"
      ],
      "paper_id": "95560",
      "pdf_url": "",
      "forum_url": "https://neurips.cc/forum?id=2024/poster/95560"
    }
  ]
}